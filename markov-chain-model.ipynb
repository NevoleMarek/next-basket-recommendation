{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Next basket recommendation - Markov chain sequential models\n",
    "\n",
    "The task of next basket recommendation is to predict a content of customers basket at their future purchase.\n",
    "\n",
    "Task has been assigned as a competition among the students who took the Algorithms of data mining course at Faculty of Information Technology @ Czech Technical University in Prague.\n",
    "\n",
    "### Scoring function\n",
    "\n",
    "The competition has been divided into 2 rounds. The difference between rounds is the used scoring function. 1st round scoring function is Jaccard similarity coefficient\n",
    "\n",
    "$J(A,B) = \\frac{ |A \\cap B| }{ |A \\cup B| }$,\n",
    "\n",
    "where A is a real basket and B is a predicted one. Final score is a mean of $J(A,B)$ over all predictions. 2nd round scoring function is Generalized Jaccard similarity coefficient over multisets\n",
    "\n",
    "$J_g(A,B) = \\frac{ \\sum_i min(a_i, b_i) }{ \\sum_i max(a_i, b_i) }$,\n",
    "\n",
    "where $A$ is a real basket and $B$ is a predicted one. $a_i$ (resp. $b_i$) is number of occurrences of $i$-th in $A$ (resp. $B$). Final score is a mean of $J_g(A,B)$ over all predictions. Generalized jaccard index takes into account cases where there is multiple occurrences of same products in one basket. Therefore the scoring function is stricter.\n",
    "\n",
    "Score in 1st rnd: ~`0.14`\n",
    "\n",
    "Position among others 1st rnd: `(not competitive enough)`\n",
    "\n",
    "Score in 2nd rnd: TBA\n",
    "\n",
    "Position among others 2nd rnd: TBA\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Unfortunately I cannot provide the data for the problem as to avoid any legal issues. The data has been provided by a external company in collaboration with the university.\n",
    "\n",
    "Even though I cannot provide the dataset I will still try to capture my thought process and all the ideas and examples will be shown.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We have been provided with 2 types of csv files.\n",
    "\n",
    "First file contains 3 columns that together create a order history. The columns are `userid`, `date` and `itemids`. Columns are self-explanatory. `userid` is an ID number of an user, `date` is a date of a purchase and `itemids` is a space separated list of product IDs. Order history files come in 2 sizes. First, the smaller one, contains 1 700 000+ rows. Second file contains total of 8 800 000+ of data.\n",
    "\n",
    "| userid \t | date       \t | itemids           \t |\n",
    "|----------|--------------|---------------------|\n",
    "| 12345  \t | 1995-30-7  \t | 11111 22222       \t |\n",
    "| 777777 \t | 2022-1-1   \t | 12314             \t |\n",
    "| 425645 \t | 2020-12-31 \t | 45646 46511 11111 \t |\n",
    "\n",
    "**(Markov chain models won't use item attributes)** Second file contains information about products. Each row represents one product. There is 25+ features about each product. Total number of products is ~1400.\n",
    "\n",
    "| productid \t | features.... \t |\n",
    "|-------------|----------------|\n",
    "| 11111     \t | features     \t |\n",
    "| 22222     \t | features     \t |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sequential Markov chain models\n",
    "\n",
    "Having $U = \\{u_1, \\ldots , u_{|U|}\\}$ a set of users and $I = \\{i_1, \\ldots , i_{|I|}\\}$ a set of items, and $\\mathcal{B^u}$ order history of user $u$, $\\mathcal{B^u} = (B^u_1, \\ldots, B^u_{t_u-1} )$, where $B^u_t \\subseteq I$. The purchase history of all users is $\\mathcal{B} = \\{ \\mathcal{B^{u_1}, \\ldots, \\mathcal{B^{u_{|U|}}}}\\}$.\n",
    "\n",
    "Models used will be discrete time Markov chains of order $1$. Meaning $p(X_t = x_t| X_{t-1} = x_{t-1}) = p(X_t = x_t| X_{t-1} = x_{t-1}, \\ldots, X_0 = x_0)$. The transitions over baskets would create transition matrix extremely large as the shape would be $ 2^{|I|} \\times 2^{|I|}$. Instead, we are gonna work with transitions between items, resulting in transition matrix of shape $ |I| \\times |I| $."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First import dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "    userid        date                        itemids\n0  7226385  2019-01-22        42203 41183 15823 39620\n1  7226385  2019-02-12              54231 14939 39462\n2  7226385  2019-03-11        15823 21028 39620 52846\n3  7226385  2019-04-03  14939 39620 27542 21028 19353\n4  7226385  2019-05-23        21028 21028 14939 15823",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userid</th>\n      <th>date</th>\n      <th>itemids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7226385</td>\n      <td>2019-01-22</td>\n      <td>42203 41183 15823 39620</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7226385</td>\n      <td>2019-02-12</td>\n      <td>54231 14939 39462</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7226385</td>\n      <td>2019-03-11</td>\n      <td>15823 21028 39620 52846</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7226385</td>\n      <td>2019-04-03</td>\n      <td>14939 39620 27542 21028 19353</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7226385</td>\n      <td>2019-05-23</td>\n      <td>21028 21028 14939 15823</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train100k.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since predictions will be based on transition matrix between items, we are gonna need map indices to product ids, to later lookup the matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df = df.assign(itemids=lambda x: x['itemids'].apply(lambda l:list(set(l.split()))))\n",
    "unique_products = df['itemids'].explode('itemids').unique()\n",
    "id_to_prod = pd.Series(unique_products, name='itemid').astype({'itemid':'uint16'})\n",
    "prod_to_id = pd.Series(id_to_prod.index.values, index=id_to_prod, name='id').astype({'id':'uint16'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data to train and validation\n",
    "\n",
    "Last basket of each user will be used as a validation basket.\n",
    "\n",
    "Rest will be used for estimating the transition matrix.\n",
    "\n",
    "Also the last basket in training of each user will be needed to use later for predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "sample = df[:10000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data_sample = (\n",
    "    sample\n",
    "        .groupby('userid')\n",
    "        .apply(lambda x: x.iloc[:-1])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "last_data_sample = (\n",
    "    data_sample\n",
    "        .groupby('userid')\n",
    "        .last()\n",
    "        .reset_index()\n",
    "    [['userid','itemids']]\n",
    ")\n",
    "\n",
    "test_sample = (\n",
    "    sample\n",
    "        .groupby('userid')\n",
    "        .last()\n",
    "        .reset_index()\n",
    "    [['userid','itemids']]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basket size\n",
    "\n",
    "For basket size mean and median of baskets is calculated for each user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "bskt_sz_sample = (\n",
    "    data_sample.explode('itemids')\n",
    "        .groupby(['userid','date'])\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .groupby('userid')\n",
    "    ['itemids']\n",
    "        .agg(mean='mean', median='median')\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimating transition matrix\n",
    "\n",
    "The probability $a_{l,i} = p(i \\in B_t| l \\in B_{t-1})$ that item $i$ is bought when $l$ was bought in previous basket will be estimated using maximum likelihood estimator. The estimation for $a_{l,i}$ given purchase history $\\mathcal{B}$ is:\n",
    "\n",
    " $\\hat{a}_{l,i} = \\hat{p}(i \\in B_t| l \\in B_{t-1}) = \\frac{\\hat{p}(i \\in B_t \\land l \\in B_{t-1})}{\\hat{p}(l \\in B_{t-1})} = \\frac{|\\{(B_t, B_{t-1}) : i \\in B_t \\land l \\in B_{t-1}\\}|}{|\\{(B_t, B_{t-1}):l\\in B_{t-1}\\}|}$\n",
    "\n",
    "**Disclaimer**\n",
    "\n",
    "For personalised MC the estimation is done over user purchase history $B^u$.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def user_transition_count(user_order_history, transition_count, item_transition_count):\n",
    "    if len(user_order_history) > 1:\n",
    "        user_order_history['next_purchase'] = user_order_history['itemids'].shift(-1)\n",
    "        for i in range(0, len(user_order_history)-1):\n",
    "            for pid in user_order_history.iloc[i]['itemids']:\n",
    "                item_transition_count[prod_to_id[int(pid)]] += 1\n",
    "                for npid in user_order_history.iloc[i]['next_purchase']:\n",
    "                    transition_count[prod_to_id[int(pid)],prod_to_id[int(npid)]] += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def transition_matrix(transition_count, item_transition_count):\n",
    "    return np.divide(\n",
    "        transition_count,\n",
    "        item_transition_count[:, None],\n",
    "        out=np.zeros_like(transition_count),\n",
    "        where=item_transition_count[:, None]!=0\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting next basket\n",
    "\n",
    "Probabilities of each item given transition matrix $B$ are calculated as follows:\n",
    "\n",
    "$p(i \\in B_t|B_{t-1}) = \\frac{1}{|B_{t-1}|} \\sum_{l \\in B_{t-1}} p(i \\in B_t| l \\in B_{t-1})$.\n",
    "\n",
    "Then top $k$ items with highest probabilities are selected as prediction, where $k$ is size of predicted basket."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def pred(userid, last_basket, trans_mat, basket_size):\n",
    "    last_basket_size = len(last_basket)\n",
    "    last_basket = map(int, last_basket)\n",
    "    probs = np.zeros((len(unique_products),))\n",
    "    for pid in last_basket:\n",
    "        probs += np.ravel(trans_mat[prod_to_id[pid]])\n",
    "    probs = probs / last_basket_size\n",
    "    probs = pd.Series(probs, name='probs').sort_values(ascending=False)\n",
    "    pred_bskt_size = int(np.floor(basket_size.loc[userid, 'mean']))\n",
    "    indexes = probs.head(pred_bskt_size).index.to_list()\n",
    "    return id_to_prod[indexes].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-personalized Markov Chain model\n",
    "\n",
    "Creation of transition matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "trans_cnt = np.zeros(shape=(len(unique_products),len(unique_products)),dtype=np.float32)\n",
    "item_trans_cnt = np.zeros(shape=(len(unique_products),),dtype=np.int32)\n",
    "data_sample.groupby('userid').apply(lambda x: user_transition_count(x[['date','itemids']],trans_cnt, item_trans_cnt))\n",
    "\n",
    "trans_mat = transition_matrix(trans_cnt, item_trans_cnt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction and evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-PMC validation score:  0.11597901222815876\n"
     ]
    }
   ],
   "source": [
    "from common import dataframe_score\n",
    "\n",
    "pred_df = last_data_sample.assign(\n",
    "    pred= lambda dataf:dataf.apply(\n",
    "        lambda row:pred(\n",
    "            row['userid'],\n",
    "            row['itemids'],\n",
    "            trans_mat,\n",
    "            bskt_sz_sample\n",
    "        ),axis=1)\n",
    ")\n",
    "\n",
    "preds = test_sample.merge(pred_df[['userid','pred']], how='inner', on='userid').rename(columns={'itemids':'product_id'})\n",
    "print('Non-PMC validation score: ',dataframe_score(preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Personalised Markov chain model\n",
    "\n",
    "The only difference from non-personalised MC is that own transition matrix is estimated for each user. Therefore transition cube is created. These matrices will be heavily sparse and for our smaller dataset of 100 000 users we would get 100 000 matrices of shape ~1400 by ~1400 which would result in nearly 1 TB of memory used. Therefore the matrices will be estimated in dense format and then transformed and stored in sparse CSR format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def create_personalised_transition_cube(order_history, personalised_trans_cube):\n",
    "    trans_cnt = np.zeros(shape=(len(unique_products),len(unique_products)),dtype=np.float32)\n",
    "    item_trans_cnt = np.zeros(shape=(len(unique_products),),dtype=np.int32)\n",
    "    user_transition_count(order_history, trans_cnt, item_trans_cnt)\n",
    "    personalised_trans_cube[order_history['userid'].iloc[0]] = sp.csr_matrix(transition_matrix(trans_cnt, item_trans_cnt))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: []\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_trans_cube = dict()\n",
    "data_sample.groupby('userid').apply(lambda group: create_personalised_transition_cube(group, per_trans_cube))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction and evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "pred_df = last_data_sample.assign(\n",
    "    pred= lambda dataf:dataf.apply(\n",
    "        lambda row:pred(\n",
    "            row['userid'],\n",
    "            row['itemids'],\n",
    "            per_trans_cube[row['userid']].todense(),\n",
    "            bskt_sz_sample\n",
    "        ),axis=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC validation score:  0.13462744041517455\n"
     ]
    }
   ],
   "source": [
    "from common import dataframe_score\n",
    "\n",
    "preds = test_sample.merge(pred_df[['userid','pred']], how='inner', on='userid').rename(columns={'itemids':'product_id'})\n",
    "print('PMC validation score: ',dataframe_score(preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Median turned out to be better than mean for the size of basket.\n",
    "\n",
    "Non-personalised MC\n",
    "\n",
    "validation score ~`0.11`.\n",
    "\n",
    "Personalised MC\n",
    "\n",
    "validation score ~`0.134`.\n",
    "\n",
    "Score on test data was `0.132`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test data predictions and scaling to full dataset\n",
    "\n",
    "Lastly we will scale to full dataset, predict on test data and format the data so that the evaluation server can process it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def test_pred_format(test):\n",
    "    test = test.rename(columns={'pred':'itemids'})\n",
    "    test['itemids'] = test['itemids'].astype(str).str.replace(r'\\[|\\]|\\,','', regex=True)\n",
    "    return test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "eval_data = pd.read_csv('data/test24k.csv').drop(columns=['itemids'])\n",
    "pdf = df.merge(eval_data[['userid']], how='inner', on='userid')\n",
    "\n",
    "data = (\n",
    "    pdf\n",
    "        .groupby('userid')\n",
    "        .apply(lambda x: x.iloc[:-1])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "last_data = (\n",
    "    pdf\n",
    "        .groupby('userid')\n",
    "        .last()\n",
    "        .reset_index()\n",
    "    [['userid','itemids']]\n",
    ")\n",
    "\n",
    "per_trans_cube = dict()\n",
    "data.groupby('userid').apply(lambda group: create_personalised_transition_cube(group, per_trans_cube))\n",
    "\n",
    "bskt_sz = (\n",
    "    pdf.explode('itemids')\n",
    "        .groupby(['userid','date'])\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .groupby('userid')\n",
    "    ['itemids']\n",
    "        .agg(mean='median')\n",
    ")\n",
    "\n",
    "pred_df = last_data.assign(\n",
    "    pred= lambda dataf:dataf.apply(\n",
    "        lambda row:pred(\n",
    "            row['userid'],\n",
    "            row['itemids'],\n",
    "            per_trans_cube[row['userid']].todense(),\n",
    "            bskt_sz\n",
    "        ),axis=1)\n",
    ")\n",
    "\n",
    "preds = eval_data.merge(pred_df[['userid','pred']], how='inner', on='userid')\n",
    "test_pred_format(preds).to_csv('data/pmctest24k.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}