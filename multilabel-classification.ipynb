{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next basket recommendation - Multilabel classification\n",
    "\n",
    "The task of next basket recommendation is to predict a content of customers basket at their future purchase.\n",
    "\n",
    "Task has been assigned as a competition among the students who took the Algorithms of data mining course at Faculty of Information Technology @ Czech Technical University in Prague.\n",
    "\n",
    "### Scoring function\n",
    "\n",
    "The competition has been divided into 2 rounds. The difference between rounds is the used scoring function. 1st round scoring function is Jaccard similarity coefficient\n",
    "\n",
    "$J(A,B) = \\frac{ |A \\cap B| }{ |A \\cup B| }$,\n",
    "\n",
    "where A is a real basket and B is a predicted one. Final score is a mean of $J(A,B)$ over all predictions. 2nd round scoring function is Generalized Jaccard similarity coefficient over multisets\n",
    "\n",
    "$J_g(A,B) = \\frac{ \\sum_i min(a_i, b_i) }{ \\sum_i max(a_i, b_i) }$,\n",
    "\n",
    "where $A$ is a real basket and $B$ is a predicted one. $a_i$ (resp. $b_i$) is number of occurrences of $i$-th in $A$ (resp. $B$). Final score is a mean of $J_g(A,B)$ over all predictions. Generalized jaccard index takes into account cases where there is multiple occurrences of same products in one basket. Therefore the scoring function is stricter.\n",
    "\n",
    "Score in 1st rnd: TBA\n",
    "\n",
    "Position among others 1st rnd: TBA\n",
    "\n",
    "Score in 2nd rnd: TBA\n",
    "\n",
    "Position among others 2nd rnd: TBA\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Unfortunately I cannot provide the data for the problem as to avoid any legal issues. The data has been provided by a external company in collaboration with the university.\n",
    "\n",
    "Even though I cannot provide the dataset I will still try to capture my thought process and all the ideas and examples will be shown.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We have been provided with 2 types of csv files.\n",
    "\n",
    "First file contains 3 columns that together create a order history. The columns are `userid`, `date` and `itemids`. Columns are self-explanatory. `userid` is an ID number of an user, `date` is a date of a purchase and `itemids` is a space separated list of product IDs. Order history files come in 2 sizes. First, the smaller one, contains 1 700 000+ rows. Second file contains total of 8 800 000+ of data.\n",
    "\n",
    "| userid \t| date       \t| itemids           \t|\n",
    "|--------\t|------------\t|-------------------\t|\n",
    "| 12345  \t| 1995-30-7  \t| 11111 22222       \t|\n",
    "| 777777 \t| 2022-1-1   \t| 12314             \t|\n",
    "| 425645 \t| 2020-12-31 \t| 45646 46511 11111 \t|\n",
    "\n",
    "Second file contains information about products. Each row represents one product. There is 25+ features about each product. Total number of products is 1426. The thoughts about feature selection for products is described later.\n",
    "\n",
    "| productid \t| features.... \t|\n",
    "|-----------\t|--------------\t|\n",
    "| 11111     \t| features     \t|\n",
    "| 22222     \t| features     \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multilabel classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First, I work with smaller dataset. Scaling to full dataset will be done in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used to preprocess the data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this project I decided to use Featuretools package for automated feature engineering. Featuretools works with typical pandas dataframes, but it's recommended to use `EntitySet` class when working with tabular data with relationships between them. From the previous look at the data We know that there is many to many relationship between order history and products table. Many to many relationships are better decomposed (normalized) to 2 one to many relationships. Now let's separate the data into 3 tables. <b>Orders</b>, <b>Orders Products</b> and <b>Products</b> tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>date</th>\n",
       "      <th>itemids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7226385</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>42203 41183 15823 39620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7226385</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>54231 14939 39462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7226385</td>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>15823 21028 39620 52846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userid        date                  itemids\n",
       "0  7226385  2019-01-22  42203 41183 15823 39620\n",
       "1  7226385  2019-02-12        54231 14939 39462\n",
       "2  7226385  2019-03-11  15823 21028 39620 52846"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train100k.csv') # Read smaller dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale later to bigger dataset I will create function to normalize the train file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_file(df, return_labels = True):\n",
    "    \"\"\" Normalize train file into Orders and Orders_Products table. \"\"\"\n",
    "    orders = ( \n",
    "        df.assign(                 \n",
    "                date=lambda x: pd.to_datetime(x['date'], infer_datetime_format=True),\n",
    "        )                \n",
    "        .rename(columns={'userid':'user_id'})\n",
    "        # Sort orders by date and create unique id for every order\n",
    "        .set_index(['date'])\n",
    "        .sort_index()\n",
    "        .assign(\n",
    "            id=range(0,len(df))\n",
    "        )\n",
    "        # Change types to save memory\n",
    "        .astype({'user_id':'uint32','id':'uint32'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    # We keep itemids column in orders dataframe to keep the relationship of order id with products\n",
    "\n",
    "    # labels will be later used to train multilabel classifiers - do not mind them for now\n",
    "    labels = orders[['id', 'itemids']]\n",
    "\n",
    "    orders_products = ( \n",
    "        orders.drop(columns=['user_id','date'])\n",
    "        # Split and explode itemids list so that each product will be on individual row, but keep order id\n",
    "        .assign(product_id=lambda x: x['itemids'].str.split())\n",
    "        .rename(columns={'id':'order_id'})\n",
    "        .drop(columns='itemids')\n",
    "        .explode('product_id')\n",
    "        .astype({'product_id':'uint16'})        \n",
    "        .assign(\n",
    "            id=lambda x: range(0,len(x)) # Creates unique id for every order product pair\n",
    "        )\n",
    "        .astype({'id':'uint32'})\n",
    "    )\n",
    "    orders_products.index = pd.RangeIndex(len(orders_products.index))\n",
    "    \n",
    "    # Lastly drop itemids from orders\n",
    "    orders = orders.drop(columns=['itemids'])\n",
    "\n",
    "    if return_labels:\n",
    "        return orders, orders_products, labels\n",
    "    return orders, orders_products\n",
    "\n",
    "\n",
    "orders, orders_products, labels = normalize_train_file(df, return_labels = True)\n",
    "\n",
    "# Uncomment to save normalized tables for later use\n",
    "# REQUIRES fastparquet package\n",
    "#orders.to_parquet(f'data/100k_orders.parquet',engine='fastparquet')\n",
    "#orders_products.to_parquet(f'data/100k_orders_products.parquet',engine='fastparquet')\n",
    "#labels.to_parquet(f'data/100k_labels.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of products file\n",
    "\n",
    "Products file takes up about 1.2 MB of space, so the main focus of preprocessing won't be to save memory or disk space by normalizing. The main focus will be on cleaning the data. I won't go into very details of the decision making of the cleaning as it isn't the subject of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>product_name</th>\n",
       "      <th>sim_product_category</th>\n",
       "      <th>sap_product_category</th>\n",
       "      <th>declaration_of_ingredients</th>\n",
       "      <th>nutriscore</th>\n",
       "      <th>country_kitchen</th>\n",
       "      <th>description</th>\n",
       "      <th>main_category_1</th>\n",
       "      <th>main_category_2</th>\n",
       "      <th>...</th>\n",
       "      <th>who_sub</th>\n",
       "      <th>when_main</th>\n",
       "      <th>when_sub</th>\n",
       "      <th>how_main</th>\n",
       "      <th>how_sub</th>\n",
       "      <th>where_main</th>\n",
       "      <th>where_sub</th>\n",
       "      <th>what_main</th>\n",
       "      <th>what_sub</th>\n",
       "      <th>gluten_lactose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53048</td>\n",
       "      <td>Wildlachs in Spinatrahmsoße</td>\n",
       "      <td>Fish</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Wildlachs 43,7%, Wasser, Vollmilch, Sahne 8,1%...</td>\n",
       "      <td>B</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>Holen Sie sich ein echtes Gourmet-Menü nach Ha...</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>{Frauentreff,\"Zeit zu Zweit\",Romantisch,weich}</td>\n",
       "      <td>{\"Mittagessen / Abendessen\"}</td>\n",
       "      <td>{\"zum Wein\",\"großer Hunger\"}</td>\n",
       "      <td>{Alltag,Festlich}</td>\n",
       "      <td>{Kochen,Hauptgang,Zwischengänge}</td>\n",
       "      <td>{\"Lunch aufwärmen\"}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Ostern,Karneval,Sommerlich,Winterlich}</td>\n",
       "      <td>{Aschermittwoch,Osterfestessen,Advent,Karfreit...</td>\n",
       "      <td>{\"Gluten \",keine_Laktose_Spuren,Laktose,keine_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11274</td>\n",
       "      <td>Hummercremesuppe</td>\n",
       "      <td>Ready-made</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Fertiggerichte</td>\n",
       "      <td>...</td>\n",
       "      <td>{Frauentreff,Muttertag,\"Zeit zu Zweit\",Romanti...</td>\n",
       "      <td>{\"Mittagessen / Abendessen\",Frühstück}</td>\n",
       "      <td>{Brunch,\"zum Wein\",Sektfrühstück,Frühschoppen}</td>\n",
       "      <td>{Party,Festlich}</td>\n",
       "      <td>{Vorspeise,Fingerfood}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Ostern,\"Frühling \",Herbstlich,Winterlich}</td>\n",
       "      <td>{Erntezeit,Thanksgiving,Osterfestessen,Weihnac...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30430</td>\n",
       "      <td>Lachscremesuppe</td>\n",
       "      <td>Ready-made</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Wasser, Sahne 18%, Fischfond (Wasser, Fischext...</td>\n",
       "      <td>C</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Fertiggerichte</td>\n",
       "      <td>...</td>\n",
       "      <td>{Schwanger,\"ohne Stücke\",Frauentreff,Romantisc...</td>\n",
       "      <td>{\"Mittagessen / Abendessen\",Frühstück}</td>\n",
       "      <td>{Brunch,\"zum Wein\",\"kleiner Hunger\",Snack}</td>\n",
       "      <td>{Party,Festlich}</td>\n",
       "      <td>{Vorspeise,Zwischengänge,Mitternacht}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"Frühling \",Ostern,Karneval,Sommerlich,Winter...</td>\n",
       "      <td>{Osterfestessen,Neujahrsfrühstück,Karfreitag,A...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemid                 product_name sim_product_category  \\\n",
       "0   53048  Wildlachs in Spinatrahmsoße                 Fish   \n",
       "1   11274             Hummercremesuppe           Ready-made   \n",
       "2   30430              Lachscremesuppe           Ready-made   \n",
       "\n",
       "  sap_product_category                         declaration_of_ingredients  \\\n",
       "0                Fisch  Wildlachs 43,7%, Wasser, Vollmilch, Sahne 8,1%...   \n",
       "1                Fisch                                                NaN   \n",
       "2                Fisch  Wasser, Sahne 18%, Fischfond (Wasser, Fischext...   \n",
       "\n",
       "  nutriscore country_kitchen  \\\n",
       "0          B     Deutschland   \n",
       "1        NaN     Deutschland   \n",
       "2          C     Deutschland   \n",
       "\n",
       "                                         description main_category_1  \\\n",
       "0  Holen Sie sich ein echtes Gourmet-Menü nach Ha...           Fisch   \n",
       "1                                                NaN           Fisch   \n",
       "2                                                NaN           Fisch   \n",
       "\n",
       "  main_category_2  ...                                            who_sub  \\\n",
       "0             NaN  ...     {Frauentreff,\"Zeit zu Zweit\",Romantisch,weich}   \n",
       "1  Fertiggerichte  ...  {Frauentreff,Muttertag,\"Zeit zu Zweit\",Romanti...   \n",
       "2  Fertiggerichte  ...  {Schwanger,\"ohne Stücke\",Frauentreff,Romantisc...   \n",
       "\n",
       "                                when_main  \\\n",
       "0            {\"Mittagessen / Abendessen\"}   \n",
       "1  {\"Mittagessen / Abendessen\",Frühstück}   \n",
       "2  {\"Mittagessen / Abendessen\",Frühstück}   \n",
       "\n",
       "                                         when_sub           how_main  \\\n",
       "0                    {\"zum Wein\",\"großer Hunger\"}  {Alltag,Festlich}   \n",
       "1  {Brunch,\"zum Wein\",Sektfrühstück,Frühschoppen}   {Party,Festlich}   \n",
       "2      {Brunch,\"zum Wein\",\"kleiner Hunger\",Snack}   {Party,Festlich}   \n",
       "\n",
       "                                 how_sub           where_main where_sub  \\\n",
       "0       {Kochen,Hauptgang,Zwischengänge}  {\"Lunch aufwärmen\"}        {}   \n",
       "1                 {Vorspeise,Fingerfood}                   {}        {}   \n",
       "2  {Vorspeise,Zwischengänge,Mitternacht}                   {}        {}   \n",
       "\n",
       "                                           what_main  \\\n",
       "0            {Ostern,Karneval,Sommerlich,Winterlich}   \n",
       "1         {Ostern,\"Frühling \",Herbstlich,Winterlich}   \n",
       "2  {\"Frühling \",Ostern,Karneval,Sommerlich,Winter...   \n",
       "\n",
       "                                            what_sub  \\\n",
       "0  {Aschermittwoch,Osterfestessen,Advent,Karfreit...   \n",
       "1  {Erntezeit,Thanksgiving,Osterfestessen,Weihnac...   \n",
       "2  {Osterfestessen,Neujahrsfrühstück,Karfreitag,A...   \n",
       "\n",
       "                                      gluten_lactose  \n",
       "0  {\"Gluten \",keine_Laktose_Spuren,Laktose,keine_...  \n",
       "1                                                 {}  \n",
       "2                                                 {}  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df = pd.read_csv('data/items.csv')\n",
    "products_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_data_columns(df, threshold = 0.5):\n",
    "    \"\"\" Drop column of data when more data than threshold percentage is missing.\"\"\"\n",
    "    return df.drop(columns=df.columns[(df.count() < threshold * len(df))].to_list())\n",
    "\n",
    "def drop_columns_matching_regex(df, rxp):\n",
    "    \"\"\" Drop columns of data where the name of the column matches with passed regular expression.\"\"\"\n",
    "    return df.drop(columns=df.columns[df.columns.str.contains(rxp, regex=True)].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "products = (\n",
    "    products_df.pipe(drop_missing_data_columns, threshold = 0.5)\n",
    "    .pipe(drop_columns_matching_regex, rxp='_main|_sub')\n",
    "    .drop(columns = ['declaration_of_ingredients', 'description', 'allergens', 'special_ingredients'])\n",
    "    .rename(columns={'itemid':'id'})\n",
    "    # Convert features with less than 60 unique values to categorical features\n",
    "    .astype(\n",
    "        {\n",
    "            **dict.fromkeys(df.columns[(df.nunique() < 60)].to_list(), 'category'),\n",
    "            **{'id':'uint16'}\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Uncomment next line to save products table\n",
    "# REQUIRES fastparquet package\n",
    "#products.to_parquet('data/products.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entity set\n",
    "\n",
    "After preprocessing and cleaning the data we can create an EntitySet and add dataframes and build relationships between them. (still working with smaller dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import featuretools.primitives as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed data or skip if in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to read data from saved files\n",
    "orders = pd.read_parquet('data/100k_orders.parquet')\n",
    "orders_products = pd.read_parquet('data/100k_orders_products.parquet')\n",
    "products = pd.read_parquet('data/products.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Decided to sample the data to determine best features. Later when best features are established I will rerun the training on full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sample 5000 users\n",
    "orders_sample = orders[orders['user_id'].isin(orders['user_id'].drop_duplicates().sample(2000))]\n",
    "orders_products_sample = orders_products[orders_products['order_id'].isin(orders_sample['id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instance of `EntitySet`. Using the `add_dataframe` method I added our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: sample_dataset\n",
       "  DataFrames:\n",
       "    orders [Rows: 34332, Columns: 3]\n",
       "    orders_products [Rows: 167293, Columns: 3]\n",
       "    products [Rows: 1426, Columns: 11]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(id='sample_dataset')\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='orders',\n",
    "    dataframe=orders_sample,\n",
    "    index='id',\n",
    "    time_index='date'\n",
    ")\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='orders_products',\n",
    "    dataframe=orders_products_sample,\n",
    "    index='id'\n",
    ")\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='products',\n",
    "    dataframe=products,\n",
    "    index='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are no relationships between the DataFrames. There is `add_relationship` method to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es.add_relationship('products', 'id', 'orders_products', 'product_id')\n",
    "es.add_relationship('orders', 'id', 'orders_products', 'order_id')\n",
    "\n",
    "# Create table of users out of orders using normalization\n",
    "es.normalize_dataframe('orders','users','user_id')\n",
    "es.add_last_time_indexes(['users'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the structure of `EntitySet` I used `plot` method. It shows format of all the dataframes and relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n -->\n<!-- Title: sample_dataset Pages: 1 -->\n<svg width=\"486pt\" height=\"446pt\"\n viewBox=\"0.00 0.00 486.00 446.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 442)\">\n<title>sample_dataset</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-442 482,-442 482,4 -4,4\"/>\n<!-- orders -->\n<g id=\"node1\" class=\"node\">\n<title>orders</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-166 0,-257 243,-257 243,-166 0,-166\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-241.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">orders (34572 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-234 243,-234 \"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-218.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">date : Datetime; time_index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-203.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-188.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-173.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- users -->\n<g id=\"node4\" class=\"node\">\n<title>users</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-76.5 243,-76.5 243,-0.5 0,-0.5\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">users (2000 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-53.5 243,-53.5 \"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-38.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id : Integer; index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">first_orders_time : Datetime; time_index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- orders&#45;&gt;users -->\n<g id=\"edge3\" class=\"edge\">\n<title>orders&#45;&gt;users</title>\n<path fill=\"none\" stroke=\"black\" d=\"M121.5,-165.59C121.5,-165.59 121.5,-86.72 121.5,-86.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"125,-86.72 121.5,-76.72 118,-86.72 125,-86.72\"/>\n<text text-anchor=\"middle\" x=\"100.5\" y=\"-129.96\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id</text>\n</g>\n<!-- orders_products -->\n<g id=\"node2\" class=\"node\">\n<title>orders_products</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"124,-346.5 124,-437.5 367,-437.5 367,-346.5 124,-346.5\"/>\n<text text-anchor=\"middle\" x=\"245.5\" y=\"-422.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">orders_products (172320 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"124,-414.5 367,-414.5 \"/>\n<text text-anchor=\"start\" x=\"132\" y=\"-399.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">order_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-384.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">product_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-369.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-354.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- orders_products&#45;&gt;orders -->\n<g id=\"edge2\" class=\"edge\">\n<title>orders_products&#45;&gt;orders</title>\n<path fill=\"none\" stroke=\"black\" d=\"M183.5,-346.45C183.5,-346.45 183.5,-267.03 183.5,-267.03\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"187,-267.03 183.5,-257.03 180,-267.03 187,-267.03\"/>\n<text text-anchor=\"middle\" x=\"144\" y=\"-310.54\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id &#45;&gt; order_id</text>\n</g>\n<!-- products -->\n<g id=\"node3\" class=\"node\">\n<title>products</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"261,-113.5 261,-309.5 478,-309.5 478,-113.5 261,-113.5\"/>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-294.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">products (1426 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"261,-286.5 478,-286.5 \"/>\n<text text-anchor=\"start\" x=\"269\" y=\"-271.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-256.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">product_name : Unknown</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-241.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sim_product_category : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-226.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sap_product_category : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-211.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nutriscore : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-196.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">country_kitchen : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-181.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">main_category_1 : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-166.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sub_category_1 : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-151.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">vegetarian : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-136.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">type_of_meat : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-121.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gluten_lactose : Categorical</text>\n</g>\n<!-- orders_products&#45;&gt;products -->\n<g id=\"edge1\" class=\"edge\">\n<title>orders_products&#45;&gt;products</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314,-346.45C314,-346.45 314,-319.55 314,-319.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"317.5,-319.55 314,-309.55 310.5,-319.55 317.5,-319.55\"/>\n<text text-anchor=\"middle\" x=\"267\" y=\"-321.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id &#45;&gt; product_id</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x147dc5ab790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EntitySet` allows to set interesting values that will later be used in `where` clauses. I add interesting values using `add_interesting_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es.add_interesting_values(max_values=8,\n",
    "                          verbose=False,\n",
    "                          dataframe_name='products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling time\n",
    "\n",
    "Since I want to calculate features for each user at certain point in time(before their order), cutoff time dataframe has to be created. Cutoff dataframe has to have targeted feature(i.e. `user_id`) and time feature(i.e. `date`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>7925639</td>\n",
       "      <td>2019-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14561</th>\n",
       "      <td>9074527</td>\n",
       "      <td>2019-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15430</th>\n",
       "      <td>8391744</td>\n",
       "      <td>2019-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>2697892</td>\n",
       "      <td>2019-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20504</th>\n",
       "      <td>3993159</td>\n",
       "      <td>2019-01-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id       time\n",
       "7069   7925639 2019-01-07\n",
       "14561  9074527 2019-01-09\n",
       "15430  8391744 2019-01-10\n",
       "18761  2697892 2019-01-11\n",
       "20504  3993159 2019-01-11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = orders_sample[['user_id','date']].rename(columns={'date':'time'})\n",
    "to_delete = cutoff.groupby('user_id', as_index=False).nth(0).index\n",
    "cutoff = cutoff.drop(to_delete)\n",
    "cutoff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of multilabels\n",
    "\n",
    "In preprocessing of training data I saved aside `itemids` for later. Here I will transform these into binary vectors of labels, which will be later used in multilabel classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_parquet('data/100k_labels.parquet', engine='fastparquet')\n",
    "productsid = products['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_labels(itemids):\n",
    "    itemids = set(itemids)    \n",
    "    label = []\n",
    "    for pid in productsid:\n",
    "        if pid in itemids:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    return np.array(label)\n",
    "\n",
    "\n",
    "test = [35593, 35593, 35593, 40265, 11232, 21868]\n",
    "res = convert_to_labels(test)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labels\n",
    "lab['itemids'] = lab['itemids'].str.split()\n",
    "lab['labels'] = lab['itemids'].apply(lambda row: convert_to_labels(row))\n",
    "lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating features and calculating the features matrix\n",
    "\n",
    "As mentioned feature engineering will be done using `featuretools` package and their `dfs` deep feature synthesis algorithm. Parameters of `dfs` can be found here: https://featuretools.alteryx.com/en/stable/generated/featuretools.dfs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_features(entityset, cutoff, training_window=None, features_only=False, n_jobs=1):    \n",
    "    return ft.dfs(entityset=entityset,\n",
    "                    target_dataframe_name='users',\n",
    "                    agg_primitives=[\n",
    "                        'max', 'mean', 'count', 'sum', 'min',\n",
    "                        'num_unique', 'mode', pr.AvgTimeBetween(unit='days'),\n",
    "                        'trend', 'std'\n",
    "                    ],\n",
    "                    trans_primitives=[\n",
    "                        'month','weekday',\n",
    "                        pr.TimeSincePrevious(unit='days')\n",
    "                    ],\n",
    "                    cutoff_time=cutoff,\n",
    "                    training_window=training_window,\n",
    "                    max_depth=3,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True,\n",
    "                    features_only=features_only,\n",
    "                    include_cutoff_time=False # Do not count on orders that came in same day\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 161 features\n",
      "EntitySet scattered to 8 workers in 33 seconds\n",
      "Elapsed: 05:48 | Progress: 100%|██████████\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVG_TIME_BETWEEN(orders.date, unit=days)</th>\n",
       "      <th>COUNT(orders)</th>\n",
       "      <th>COUNT(orders_products)</th>\n",
       "      <th>MONTH(first_orders_time)</th>\n",
       "      <th>TIME_SINCE_PREVIOUS(first_orders_time, unit=days)</th>\n",
       "      <th>WEEKDAY(first_orders_time)</th>\n",
       "      <th>MAX(orders.COUNT(orders_products))</th>\n",
       "      <th>MAX(orders.TIME_SINCE_PREVIOUS(date, unit=days))</th>\n",
       "      <th>MEAN(orders.COUNT(orders_products))</th>\n",
       "      <th>MEAN(orders.TIME_SINCE_PREVIOUS(date, unit=days))</th>\n",
       "      <th>...</th>\n",
       "      <th>COUNT(orders_products WHERE products.sap_product_category = Backwaren)</th>\n",
       "      <th>COUNT(orders_products WHERE products.vegetarian = f)</th>\n",
       "      <th>COUNT(orders_products WHERE products.sub_category_1 = Pfannengerichte)</th>\n",
       "      <th>COUNT(orders_products WHERE products.nutriscore = D)</th>\n",
       "      <th>COUNT(orders_products WHERE products.gluten_lactose = {keine_Laktose,keine_Laktose_Spuren,kein_Weizen,keine_Weizen_Spuren,keine_Gluten_Spuren,\"kein_Gluten \"})</th>\n",
       "      <th>COUNT(orders_products WHERE products.sim_product_category = Meat)</th>\n",
       "      <th>COUNT(orders_products WHERE products.main_category_1 = Fertiggerichte)</th>\n",
       "      <th>COUNT(orders_products WHERE products.main_category_1 = Fisch)</th>\n",
       "      <th>COUNT(orders_products WHERE products.gluten_lactose = {\"Gluten \",keine_Laktose_Spuren,kein_Weizen,Laktose,keine_Weizen_Spuren,keine_Gluten_Spuren})</th>\n",
       "      <th>COUNT(orders_products WHERE products.country_kitchen = Italien)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7925639</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9074527</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8391744</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697892</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AVG_TIME_BETWEEN(orders.date, unit=days)  COUNT(orders)  \\\n",
       "user_id                                                            \n",
       "7925639                                       NaN              1   \n",
       "9074527                                       NaN              1   \n",
       "8391744                                       NaN              1   \n",
       "2697892                                       NaN              1   \n",
       "3993159                                       NaN              1   \n",
       "\n",
       "         COUNT(orders_products) MONTH(first_orders_time)  \\\n",
       "user_id                                                    \n",
       "7925639                       2                        1   \n",
       "9074527                       4                        1   \n",
       "8391744                       8                        1   \n",
       "2697892                       4                        1   \n",
       "3993159                       4                        1   \n",
       "\n",
       "         TIME_SINCE_PREVIOUS(first_orders_time, unit=days)  \\\n",
       "user_id                                                      \n",
       "7925639                                                NaN   \n",
       "9074527                                                NaN   \n",
       "8391744                                                NaN   \n",
       "2697892                                                7.0   \n",
       "3993159                                                1.0   \n",
       "\n",
       "        WEEKDAY(first_orders_time)  MAX(orders.COUNT(orders_products))  \\\n",
       "user_id                                                                  \n",
       "7925639                          3                                 2.0   \n",
       "9074527                          1                                 4.0   \n",
       "8391744                          0                                 8.0   \n",
       "2697892                          2                                 4.0   \n",
       "3993159                          3                                 4.0   \n",
       "\n",
       "         MAX(orders.TIME_SINCE_PREVIOUS(date, unit=days))  \\\n",
       "user_id                                                     \n",
       "7925639                                               NaN   \n",
       "9074527                                               NaN   \n",
       "8391744                                               NaN   \n",
       "2697892                                               7.0   \n",
       "3993159                                               1.0   \n",
       "\n",
       "         MEAN(orders.COUNT(orders_products))  \\\n",
       "user_id                                        \n",
       "7925639                                  2.0   \n",
       "9074527                                  4.0   \n",
       "8391744                                  8.0   \n",
       "2697892                                  4.0   \n",
       "3993159                                  4.0   \n",
       "\n",
       "         MEAN(orders.TIME_SINCE_PREVIOUS(date, unit=days))  ...  \\\n",
       "user_id                                                     ...   \n",
       "7925639                                                NaN  ...   \n",
       "9074527                                                NaN  ...   \n",
       "8391744                                                NaN  ...   \n",
       "2697892                                                7.0  ...   \n",
       "3993159                                                1.0  ...   \n",
       "\n",
       "         COUNT(orders_products WHERE products.sap_product_category = Backwaren)  \\\n",
       "user_id                                                                           \n",
       "7925639                                                  0                        \n",
       "9074527                                                  0                        \n",
       "8391744                                                  0                        \n",
       "2697892                                                  0                        \n",
       "3993159                                                  0                        \n",
       "\n",
       "         COUNT(orders_products WHERE products.vegetarian = f)  \\\n",
       "user_id                                                         \n",
       "7925639                                                  1      \n",
       "9074527                                                  2      \n",
       "8391744                                                  4      \n",
       "2697892                                                  3      \n",
       "3993159                                                  3      \n",
       "\n",
       "        COUNT(orders_products WHERE products.sub_category_1 = Pfannengerichte)  \\\n",
       "user_id                                                                          \n",
       "7925639                                                  0                       \n",
       "9074527                                                  0                       \n",
       "8391744                                                  1                       \n",
       "2697892                                                  0                       \n",
       "3993159                                                  0                       \n",
       "\n",
       "        COUNT(orders_products WHERE products.nutriscore = D)  \\\n",
       "user_id                                                        \n",
       "7925639                                                  0     \n",
       "9074527                                                  0     \n",
       "8391744                                                  0     \n",
       "2697892                                                  1     \n",
       "3993159                                                  2     \n",
       "\n",
       "         COUNT(orders_products WHERE products.gluten_lactose = {keine_Laktose,keine_Laktose_Spuren,kein_Weizen,keine_Weizen_Spuren,keine_Gluten_Spuren,\"kein_Gluten \"})  \\\n",
       "user_id                                                                                                                                                                   \n",
       "7925639                                                  1                                                                                                                \n",
       "9074527                                                  3                                                                                                                \n",
       "8391744                                                  4                                                                                                                \n",
       "2697892                                                  1                                                                                                                \n",
       "3993159                                                  1                                                                                                                \n",
       "\n",
       "         COUNT(orders_products WHERE products.sim_product_category = Meat)  \\\n",
       "user_id                                                                      \n",
       "7925639                                                  0                   \n",
       "9074527                                                  0                   \n",
       "8391744                                                  0                   \n",
       "2697892                                                  0                   \n",
       "3993159                                                  2                   \n",
       "\n",
       "         COUNT(orders_products WHERE products.main_category_1 = Fertiggerichte)  \\\n",
       "user_id                                                                           \n",
       "7925639                                                  0                        \n",
       "9074527                                                  0                        \n",
       "8391744                                                  1                        \n",
       "2697892                                                  0                        \n",
       "3993159                                                  1                        \n",
       "\n",
       "         COUNT(orders_products WHERE products.main_category_1 = Fisch)  \\\n",
       "user_id                                                                  \n",
       "7925639                                                  1               \n",
       "9074527                                                  1               \n",
       "8391744                                                  2               \n",
       "2697892                                                  0               \n",
       "3993159                                                  0               \n",
       "\n",
       "         COUNT(orders_products WHERE products.gluten_lactose = {\"Gluten \",keine_Laktose_Spuren,kein_Weizen,Laktose,keine_Weizen_Spuren,keine_Gluten_Spuren})  \\\n",
       "user_id                                                                                                                                                        \n",
       "7925639                                                  0                                                                                                     \n",
       "9074527                                                  0                                                                                                     \n",
       "8391744                                                  0                                                                                                     \n",
       "2697892                                                  0                                                                                                     \n",
       "3993159                                                  0                                                                                                     \n",
       "\n",
       "         COUNT(orders_products WHERE products.country_kitchen = Italien)  \n",
       "user_id                                                                   \n",
       "7925639                                                  0                \n",
       "9074527                                                  0                \n",
       "8391744                                                  0                \n",
       "2697892                                                  3                \n",
       "3993159                                                  0                \n",
       "\n",
       "[5 rows x 161 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm ,features = calculate_features(es, cutoff, features_only=False, n_jobs=-1)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that some generated features will have almost all of its values equal to null or some single value. To remove these features `featuretools` has functions named `remove_highly_null_features` and `remove_single_value_features`. To remove highly correlated features I used `remove_highly_correlated_featrues` function. Further more categorical features need to be encoded using one hot encoding. This is done using `encode_features` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after encoding (32332, 357)\n",
      "Shape after preprocessing (32332, 307)\n"
     ]
    }
   ],
   "source": [
    "fm_enc, features_enc = ft.encode_features(fm, features)\n",
    "print(f'Shape after encoding {fm_enc.shape}')\n",
    "fm_enc = ft.selection.remove_highly_null_features(fm_enc, pct_null_threshold=0.8)\n",
    "fm_enc = ft.selection.remove_single_value_features(fm_enc)\n",
    "fm_enc = ft.selection.remove_highly_correlated_features(fm_enc, pct_corr_threshold = 0.9)\n",
    "print(f'Shape after preprocessing {fm_enc.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling to larger dataset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac414b804d36e4c085e24a0481bd4396ee16fd0a7c5871f3d68efff1fa8b72ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}