{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Next basket recommendation - Top frequency\n",
    "\n",
    "The task of next basket recommendation is to predict a content of customers basket at their future purchase.\n",
    "\n",
    "Task has been assigned as a competition among the students who took the Algorithms of data mining course at Faculty of Information Technology @ Czech Technical University in Prague.\n",
    "\n",
    "### Scoring function\n",
    "\n",
    "The competition has been divided into 2 rounds. The difference between rounds is the used scoring function. 1st round scoring function is Jaccard similarity coefficient\n",
    "\n",
    "$J(A,B) = \\frac{ |A \\cap B| }{ |A \\cup B| }$,\n",
    "\n",
    "where A is a real basket and B is a predicted one. Final score is a mean of $J(A,B)$ over all predictions. 2nd round scoring function is Generalized Jaccard similarity coefficient over multisets\n",
    "\n",
    "$J_g(A,B) = \\frac{ \\sum_i min(a_i, b_i) }{ \\sum_i max(a_i, b_i) }$,\n",
    "\n",
    "where $A$ is a real basket and $B$ is a predicted one. $a_i$ (resp. $b_i$) is number of occurrences of $i$-th in $A$ (resp. $B$). Final score is a mean of $J_g(A,B)$ over all predictions. Generalized jaccard index takes into account cases where there is multiple occurrences of same products in one basket. Therefore the scoring function is stricter.\n",
    "\n",
    "Score in 1st rnd: 0.188  Pos: 7/46\n",
    "\n",
    "Score in 2nd rnd: TBA\n",
    "\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Unfortunately I cannot provide the data for the problem as to avoid any legal issues. The data has been provided by a external company in collaboration with the university.\n",
    "\n",
    "Even though I cannot provide the dataset I will still try to capture my thought process and all the ideas and examples will be shown.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We have been provided with 2 csv files.\n",
    "\n",
    "First file holds basket history. It contains 3 columns. The columns are `userid`, `date` and `itemids`. Columns are self-explanatory. `userid` is an ID number of an user, `date` is a date of a purchase and `itemids` is a space separated list of product IDs. Basket history files come in 2 sizes. First, the smaller one, contains 1 700 000+ rows. Second file contains total of 8 800 000+ of data.\n",
    "\n",
    "| userid \t | date       \t | itemids           \t |\n",
    "|----------|--------------|---------------------|\n",
    "| 12345  \t | 1995-30-7  \t | 11111 22222       \t |\n",
    "| 777777 \t | 2022-1-1   \t | 12314             \t |\n",
    "| 425645 \t | 2020-12-31 \t | 45646 46511 11111 \t |\n",
    "\n",
    "Second file contains information about products. Each row represents one product. There is 25+ features about each product. Total number of products is around 1500.\n",
    "\n",
    "| productid \t | features.... \t |\n",
    "|-------------|----------------|\n",
    "| 11111     \t | features     \t |\n",
    "| 22222     \t | features     \t |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TOP frequency\n",
    "\n",
    "This notebook will focus purely on TOP recommendation methods, which are broadly used as a baseline methods for next basket recommendation.\n",
    "\n",
    "First we will preprocess and prepare data for prediction. After that series of methods and improvements will be shown and evaluated on sample data. Lastly we will scale to full dataset and predict for test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "\n",
    "`common.py` stores functions that would otherwise clutter the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from common import expl_ratio\n",
    "from common import dataframe_prediction, dataframe_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First import smaller dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "    userid        date                        itemids\n0  7226385  2019-01-22        42203 41183 15823 39620\n1  7226385  2019-02-12              54231 14939 39462\n2  7226385  2019-03-11        15823 21028 39620 52846\n3  7226385  2019-04-03  14939 39620 27542 21028 19353\n4  7226385  2019-05-23        21028 21028 14939 15823",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userid</th>\n      <th>date</th>\n      <th>itemids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7226385</td>\n      <td>2019-01-22</td>\n      <td>42203 41183 15823 39620</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7226385</td>\n      <td>2019-02-12</td>\n      <td>54231 14939 39462</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7226385</td>\n      <td>2019-03-11</td>\n      <td>15823 21028 39620 52846</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7226385</td>\n      <td>2019-04-03</td>\n      <td>14939 39620 27542 21028 19353</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7226385</td>\n      <td>2019-05-23</td>\n      <td>21028 21028 14939 15823</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train100k.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(1711877, 3)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "\n",
    "In first step we will convert date to datetime and split ids of bought items for entire dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(\n",
    "        date=lambda x: pd.to_datetime(x['date'], infer_datetime_format=True),\n",
    "        product_id=lambda x: x['itemids'].str.split(),\n",
    "    )\n",
    "    .drop(columns=['itemids'])\n",
    "    .astype({'userid':'uint32'})\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Smaller dataset contains 1.7 mil. baskets. We will take a sample of data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "sample = df[:50000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Baskets for each user were already sorted by date for us. As a validation dataset we will use last purchased basket of every user. The rest of the baskets will be used for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2942, 3)\n",
      "(47058, 3)\n"
     ]
    }
   ],
   "source": [
    "valid_df = (\n",
    "    sample\n",
    "        .groupby('userid')\n",
    "        .last()\n",
    "        .reset_index()\n",
    ")\n",
    "train_df = (\n",
    "    sample\n",
    "        .groupby('userid')\n",
    "        .apply(lambda x: x.iloc[:-1])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "print(valid_df.shape)\n",
    "print(train_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next step we will remove duplicate items in each basket. Then we will take each basket of each user and calculate month difference from the validation basket. As this will be later used. Lastly we will explode baskets so that each bought product is in own row."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "    userid       date  product_id  date_last  monthdiff\n0  1002915 2020-04-02       12043 2020-12-11   8.312286\n1  1002915 2020-04-02       44895 2020-12-11   8.312286\n2  1002915 2020-04-02       14291 2020-12-11   8.312286\n3  1002915 2020-04-02       39917 2020-12-11   8.312286\n4  1002915 2020-04-02       18339 2020-12-11   8.312286",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userid</th>\n      <th>date</th>\n      <th>product_id</th>\n      <th>date_last</th>\n      <th>monthdiff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1002915</td>\n      <td>2020-04-02</td>\n      <td>12043</td>\n      <td>2020-12-11</td>\n      <td>8.312286</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002915</td>\n      <td>2020-04-02</td>\n      <td>44895</td>\n      <td>2020-12-11</td>\n      <td>8.312286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1002915</td>\n      <td>2020-04-02</td>\n      <td>14291</td>\n      <td>2020-12-11</td>\n      <td>8.312286</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1002915</td>\n      <td>2020-04-02</td>\n      <td>39917</td>\n      <td>2020-12-11</td>\n      <td>8.312286</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1002915</td>\n      <td>2020-04-02</td>\n      <td>18339</td>\n      <td>2020-12-11</td>\n      <td>8.312286</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_train_df = (\n",
    "        train_df\n",
    "        .merge(valid_df[['userid','date']], how='inner', on='userid', suffixes=('','_last'))\n",
    "        .assign(\n",
    "            product_id=lambda x: x['product_id'].apply(lambda l:list(set(l))),\n",
    "            monthdiff=lambda x: (x['date_last'] - x['date'])/np.timedelta64(1, 'M')\n",
    "        )\n",
    "        .explode('product_id')\n",
    "        .astype({'product_id':'uint16'})\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "exploded_train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basket size\n",
    "\n",
    "For basket size mean and median of baskets is calculated for each user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "              mean  median\nuserid                    \n1002915   3.272727     3.0\n1007942  10.000000    11.0\n1008516   2.125000     2.0\n1009684   5.615385     5.0\n1013330   1.500000     1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>median</th>\n    </tr>\n    <tr>\n      <th>userid</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1002915</th>\n      <td>3.272727</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1007942</th>\n      <td>10.000000</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>1008516</th>\n      <td>2.125000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1009684</th>\n      <td>5.615385</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1013330</th>\n      <td>1.500000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basket_size = (\n",
    "    exploded_train_df[['userid', 'product_id', 'date']]\n",
    "            .groupby(['userid','date'])\n",
    "            .count()\n",
    "            .reset_index()\n",
    "            .groupby('userid')\n",
    "            ['product_id']\n",
    "            .agg(mean='mean', median='median')\n",
    ")\n",
    "basket_size.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting next basket\n",
    "\n",
    "Now we will move on predicting next baskets.\n",
    "\n",
    "Number of methods and improvements will be shown and discussed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TOP Generalized frequencies\n",
    "\n",
    "For each product count of purchases is calculated. Prediction afterwards is simple. Take top-$K$ bought products, where $K$ is the basket size, and return as prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "            freq\nproduct_id      \n10985       1559\n54684       1475\n31758       1444\n43437       1306\n47139       1289",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>freq</th>\n    </tr>\n    <tr>\n      <th>product_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10985</th>\n      <td>1559</td>\n    </tr>\n    <tr>\n      <th>54684</th>\n      <td>1475</td>\n    </tr>\n    <tr>\n      <th>31758</th>\n      <td>1444</td>\n    </tr>\n    <tr>\n      <th>43437</th>\n      <td>1306</td>\n    </tr>\n    <tr>\n      <th>47139</th>\n      <td>1289</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfreqs = (\n",
    "    exploded_train_df[['product_id', 'date']]\n",
    "        .groupby('product_id')\n",
    "        .agg(freq=('date','count'))\n",
    "        .astype({'freq':'uint32'})\n",
    "        .sort_values(by='freq', ascending=False)\n",
    ")\n",
    "gfreqs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G-topfreq:  0.018057837267940368\n"
     ]
    }
   ],
   "source": [
    "gfreq_test = dataframe_prediction(valid_df, 'gfreq', gfreqs, basket_size.loc[:,'median'])\n",
    "print('G-topfreq: ', dataframe_score(gfreq_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected this model is very weak and doesn't capture different trends among customers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TOP Personalized frequencies\n",
    "\n",
    "Huge improvement is gained when counts of purchases are calculated based on each users basket history. Therefore predictions are solely based on user's past preferences. Disadvantage of this approach is exploration of new products is not possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                    freq\nuserid  product_id      \n1002915 18339          4\n        20262          4\n        55167          3\n        14291          2\n        34720          2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>freq</th>\n    </tr>\n    <tr>\n      <th>userid</th>\n      <th>product_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1002915</th>\n      <th>18339</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>20262</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>55167</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14291</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>34720</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfreqs = (\n",
    "    exploded_train_df[['userid','product_id','date']]\n",
    "        .groupby(['userid','product_id'])\n",
    "        .agg(freq=('date','count'))\n",
    "        .astype({'freq':'uint32'})\n",
    "        .sort_values(by=['userid','freq'], ascending=[True,False])\n",
    ")\n",
    "pfreqs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-topfreq:  0.17351118426603324\n"
     ]
    }
   ],
   "source": [
    "pfreq_test = dataframe_prediction(valid_df, 'pfreq', pfreqs, basket_size.loc[:,'median'])\n",
    "print('P-topfreq: ', dataframe_score(pfreq_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In comparison to Generalized TOP method the score of personalized TOP frequencies is about 10x higher for the sample used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding weighted frequencies\n",
    "\n",
    "Back in preprocessing we calculated month difference between users past baskets and validation basket. The expectation is that users taste is changing throughout the time and that products not recently bought should be predicted less. To achieve this, frequencies are not just simply incremented for each occurence. Instead occurance is weighted by $e^{-\\frac{x}{a}$, where $x$ is the timedelta (i.e. month difference ) and $a \\in  [0, +\\infty )$ is a hyperparameter. This is so called exponential decay."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def wpfrequencies(data_df, a):\n",
    "    return (\n",
    "        data_df[['userid','product_id','monthdiff']]\n",
    "            .assign(weight=lambda x:np.exp(-x['monthdiff']/a))\n",
    "            .groupby(['userid','product_id'])\n",
    "            .agg(freq=('weight','sum'))\n",
    "            .astype({'freq':'float32'})\n",
    "            .sort_values(by=['userid','freq'], ascending=[True,False])\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameter tuning $a$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with a=9.156626506024097 WP-topfreq score = 0.18658268917932255\n"
     ]
    }
   ],
   "source": [
    "a = np.linspace(5, 20, num=250)\n",
    "param_a_scores = {}\n",
    "\n",
    "for param in a:\n",
    "    wpfreqs = wpfrequencies(exploded_train_df, param)\n",
    "    wpfreq_test = dataframe_prediction(valid_df, 'wpfreq', wpfreqs, basket_size.loc[:,'median'])\n",
    "    param_a_scores[param] = dataframe_score(wpfreq_test)\n",
    "\n",
    "best_a = max(param_a_scores, key=param_a_scores.get)\n",
    "print(f'with a={best_a} WP-topfreq score = {param_a_scores[best_a]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this sample the score improved by around 10%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generalized + weighted personalized frequencies\n",
    "\n",
    "Another idea to improve predictions was to measure exploration of users in their last $n$ baskets. Exploration meaning percentage of items that they bought for the first time in last $n$ baskets. And then from the percentage determine expected number of never before bought items in next basket. The calculated number of items is then selected from generalized TOP frequencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def build_rpt_expl_df(ddf, n):\n",
    "    return (\n",
    "        ddf\n",
    "            .groupby('userid')\n",
    "            .agg(exploration=('product_id',lambda x:expl_ratio(x,n)))\n",
    "            .assign(\n",
    "            repetition=lambda x:1-x['exploration']\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameter tuning $n$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with n=0 GWP-topfreq score = 0.18658268917932255\n"
     ]
    }
   ],
   "source": [
    "n = range(0,10)\n",
    "wpfreqs = wpfrequencies(exploded_train_df, best_a)\n",
    "param_n_scores = {}\n",
    "\n",
    "for param in n:\n",
    "    rpt_expl_df = build_rpt_expl_df(train_df, param)\n",
    "    gwpfreq_test = dataframe_prediction(valid_df, 'gpfreq', (wpfreqs,gfreqs), basket_size.loc[:,'median'], rpt_expl_df)\n",
    "    param_n_scores[param] = dataframe_score(gwpfreq_test)\n",
    "\n",
    "best_n = max(param_n_scores, key=param_n_scores.get)\n",
    "print(f'with n={best_n} GWP-topfreq score = {param_n_scores[best_n]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best result with n=0 means that this idea didn't improve the predicting power of the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing often unsuccessfully predicted items\n",
    "\n",
    "Idea behind is to measure accuracy of predictions for each item and simply not predict items that are often predicted incorrectly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def count_successful_predictions(prediction, product_prediction_accuracy):\n",
    "    for prod in prediction['pred']:\n",
    "        product_prediction_accuracy.loc[prod,'pred_cnt'] += 1\n",
    "        if prod in map(int,prediction['product_id']):\n",
    "            product_prediction_accuracy.loc[prod,'pred_success'] += 1\n",
    "\n",
    "def remove_banned(predicted_basket, banned):\n",
    "    return [prod for prod in predicted_basket if prod not in banned]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score before removing banned items 0.18658268917932255\n"
     ]
    }
   ],
   "source": [
    "wpfreqs = wpfrequencies(exploded_train_df, best_a)\n",
    "wpfreq_test = dataframe_prediction(valid_df, 'wpfreq', wpfreqs, basket_size.loc[:,'median'])\n",
    "print('Score before removing banned items',dataframe_score(wpfreq_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will calculate item predicition accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "pred_acc = pd.DataFrame(index=gfreqs.index)\n",
    "pred_acc[['pred_cnt', 'pred_success']] = 0\n",
    "wpfreq_test.apply(lambda row: count_successful_predictions(row[['product_id','pred']], pred_acc), axis=1)\n",
    "pred_acc['acc'] = pred_acc['pred_success']/pred_acc['pred_cnt']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameter tuning $p$, where $p$ is a threshold for accuracy under which items are considered as banned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with p=0.07878787878787878 score = 0.19376484943082484\n"
     ]
    }
   ],
   "source": [
    "p = np.linspace(0,0.2,100)\n",
    "param_p_scores = {}\n",
    "\n",
    "for param_p in p:\n",
    "    banned_prods = set(pred_acc.query('acc < @param_p').index.tolist())\n",
    "    tmpdf = wpfreq_test.copy()\n",
    "    tmpdf['pred'] = tmpdf['pred'].map(lambda row: remove_banned(row, banned_prods))\n",
    "    param_p_scores[param_p] = dataframe_score(tmpdf)\n",
    "\n",
    "best_p = max(param_p_scores, key=param_p_scores.get)\n",
    "print(f'with p={best_p} score = {param_p_scores[best_p]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This approach again improved the score by around 10%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Median turned out to be better than mean for the size of basket.\n",
    "\n",
    "Generalized TOP not really useful model but valuable stepping stone.\n",
    "\n",
    "validation score ~`0.018`\n",
    "\n",
    "Personalized TOP gave huge improvement and is basis for my best solution.\n",
    "\n",
    "validation score ~`0.175`\n",
    "\n",
    "Weighted personalized TOP with the hypertuned parameter `a` it turns out baskets 12 months old are around 3.5x less important and 24 months old around 10x less important than 1 month old baskets.\n",
    "\n",
    "validation score ~`0.185`\n",
    "\n",
    "Exploration attempt was not successful.\n",
    "\n",
    "Removing often unsuccessfully predicted items again slightly improved prediction score. To my understanding these banned items were once heavily bought but not anymore.\n",
    "\n",
    "validation score ~`0.193`\n",
    "\n",
    "Score on test data was `0.188`.\n",
    "\n",
    "Position `7/45`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test data predictions and scaling to full dataset\n",
    "\n",
    "Lastly we will scale to full dataset, predict on test data and format the data so that the evaluation server can process it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def test_pred_format(test):\n",
    "    test = test.rename(columns={'pred':'itemids'})\n",
    "    test['itemids'] = test['itemids'].astype(str).str.replace(r'\\[|\\]|\\,','', regex=True)\n",
    "    return test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "test_df = (\n",
    "        pd.read_csv('data/test24k.csv')\n",
    "        .drop(columns=['itemids'])\n",
    "        .assign(date=lambda x: pd.to_datetime(x['date'], infer_datetime_format=True))\n",
    ")\n",
    "\n",
    "exploded_train_df = (\n",
    "    df\n",
    "        .merge(test_df[['userid','date']], how='inner', on='userid', suffixes=('','_last'))\n",
    "        .assign(\n",
    "        product_id=lambda x: x['product_id'].apply(lambda l:list(set(l))),\n",
    "        monthdiff=lambda x: (x['date_last'] - x['date'])/np.timedelta64(1, 'M')\n",
    "    )\n",
    "        .explode('product_id')\n",
    "        .astype({'product_id':'uint16'})\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "basket_size = (\n",
    "    exploded_train_df[['userid', 'product_id', 'date']]\n",
    "        .groupby(['userid','date'])\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .groupby('userid')\n",
    "    ['product_id']\n",
    "        .agg(mean='mean', std='std', median='median')\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "banned_prods = set(pred_acc.query('acc < @best_p').index.tolist())\n",
    "tmpdf = dataframe_prediction(test_df, 'wpfreq', wpfrequencies(exploded_train_df, best_a), basket_size.loc[:,'median'])\n",
    "tmpdf['pred'] = tmpdf['pred'].map(lambda row: remove_banned(row, banned_prods))\n",
    "test_pred_format(tmpdf).to_csv('data/pred24k.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}