{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next basket recommendation\n",
    "\n",
    "The task of next basket recommendation is to predict a content of customers basket at their future purchase.\n",
    "\n",
    "Task has been assigned as a competition among the students who took the Algorithms of data mining course at Faculty of Information Technology @ Czech Technical University in Prague.\n",
    "\n",
    "### Scoring function\n",
    "\n",
    "The competition has been divided into 2 rounds. The difference between rounds is the used scoring function. 1st round scoring function is Jaccard similarity coefficient\n",
    "\n",
    "$J(A,B) = \\frac{ |A \\cap B| }{ |A \\cup B| }$,\n",
    "\n",
    "where A is a real basket and B is a predicted one. Final score is a mean of $J(A,B)$ over all predictions. 2nd round scoring function is Generalized Jaccard similarity coefficient over multisets\n",
    "\n",
    "$J_g(A,B) = \\frac{ \\sum_i min(a_i, b_i) }{ \\sum_i max(a_i, b_i) }$,\n",
    "\n",
    "where $A$ is a real basket and $B$ is a predicted one. $a_i$ (resp. $b_i$) is number of occurrences of $i$-th in $A$ (resp. $B$). Final score is a mean of $J_g(A,B)$ over all predictions. Generalized jaccard index takes into account cases where there is multiple occurrences of same products in one basket. Therefore the scoring function is stricter.\n",
    "\n",
    "Score in 1st rnd: TBA\n",
    "\n",
    "Position among others 1st rnd: TBA\n",
    "\n",
    "Score in 2nd rnd: TBA\n",
    "\n",
    "Position among others 2nd rnd: TBA\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Unfortunately I cannot provide the data for the problem as to avoid any legal issues. The data has been provided by a external company in collaboration with the university.\n",
    "\n",
    "\n",
    "\n",
    "Even though I cannot provide the dataset I will still try to capture my thought process and all the ideas and examples will be shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We have been provided with 2 types of csv files.\n",
    "\n",
    "First file contains 3 columns that together create a order history. The columns are `userid`, `date` and `itemids`. Columns are self-explanatory. `userid` is an ID number of an user, `date` is a date of a purchase and `itemids` is a space separated list of product IDs. Order history files come in 2 sizes. First, the smaller one, contains 1 700 000+ rows. Second file contains total of 8 800 000+ of data.\n",
    "\n",
    "| userid \t| date       \t| itemids           \t|\n",
    "|--------\t|------------\t|-------------------\t|\n",
    "| 12345  \t| 1995-30-7  \t| 11111 22222       \t|\n",
    "| 777777 \t| 2022-1-1   \t| 12314             \t|\n",
    "| 425645 \t| 2020-12-31 \t| 45646 46511 11111 \t|\n",
    "\n",
    "Second file contains information about products. Each row represents one product. There is 25+ features about each product. Total number of products is 1426. The thoughts about feature selection for products is described later.\n",
    "\n",
    "| productid \t| features.... \t|\n",
    "|-----------\t|--------------\t|\n",
    "| 11111     \t| features     \t|\n",
    "| 22222     \t| features     \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First, I work with smaller dataset. Scaling to full dataset will be done in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used to preprocess the data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this project I decided to use Featuretools package for automated feature engineering. Featuretools works with typical pandas dataframes, but it's recommended to use `EntitySet` class when working with tabular data with relationships between them. From the previous look at the data We know that there is many to many relationship between order history and products table. Many to many relationships are better decomposed (normalized) to 2 one to many relationships. Now let's separate the data into 3 tables. <b>Orders</b>, <b>Orders Products</b> and <b>Products</b> tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    userid        date                  itemids\n0  7226385  2019-01-22  42203 41183 15823 39620\n1  7226385  2019-02-12        54231 14939 39462\n2  7226385  2019-03-11  15823 21028 39620 52846",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userid</th>\n      <th>date</th>\n      <th>itemids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7226385</td>\n      <td>2019-01-22</td>\n      <td>42203 41183 15823 39620</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7226385</td>\n      <td>2019-02-12</td>\n      <td>54231 14939 39462</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7226385</td>\n      <td>2019-03-11</td>\n      <td>15823 21028 39620 52846</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train100k.csv') # Read smaller dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale later to bigger dataset I will create function to normalize the train file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_file(df, return_labels = True):\n",
    "    \"\"\" Normalize train file into Orders and Orders_Products table. \"\"\"\n",
    "    orders = ( \n",
    "        df.assign(                 \n",
    "                date=lambda x: pd.to_datetime(x['date'], infer_datetime_format=True),\n",
    "        )                \n",
    "        .rename(columns={'userid':'user_id'})\n",
    "        # Sort orders by date and create unique id for every order\n",
    "        .set_index(['date'])\n",
    "        .sort_index()\n",
    "        .assign(\n",
    "            id=range(0,len(df))\n",
    "        )\n",
    "        # Change types to save memory\n",
    "        .astype({'user_id':'uint32','id':'uint32'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    # We keep itemids column in orders dataframe to keep the relationship of order id with products\n",
    "\n",
    "    # labels will be later used to train multilabel classifiers - do not mind them for now\n",
    "    labels = orders[['id', 'itemids']]\n",
    "\n",
    "    orders_products = ( \n",
    "        orders.drop(columns=['user_id','date'])\n",
    "        # Split and explode itemids list so that each product will be on individual row, but keep order id\n",
    "        .assign(product_id=lambda x: x['itemids'].str.split())\n",
    "        .rename(columns={'id':'order_id'})\n",
    "        .drop(columns='itemids')\n",
    "        .explode('product_id')\n",
    "        .astype({'product_id':'uint16'})        \n",
    "        .assign(\n",
    "            id=lambda x: range(0,len(x)) # Creates unique id for every order product pair\n",
    "        )\n",
    "        .astype({'id':'uint32'})\n",
    "    )\n",
    "    orders_products.index = pd.RangeIndex(len(orders_products.index))\n",
    "    \n",
    "    # Lastly drop itemids from orders\n",
    "    orders = orders.drop(columns=['itemids'])\n",
    "\n",
    "    if return_labels:\n",
    "        return orders, orders_products, labels\n",
    "    return orders, orders_products\n",
    "\n",
    "\n",
    "orders, orders_products, labels = normalize_train_file(df, return_labels = True)\n",
    "\n",
    "# Uncomment to save normalized tables for later use\n",
    "# REQUIRES fastparquet package\n",
    "#orders.to_parquet(f'data/100k_orders.parquet',engine='fastparquet')\n",
    "#orders_products.to_parquet(f'data/100k_orders_products.parquet',engine='fastparquet')\n",
    "#labels.to_parquet(f'data/100k_labels.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of products file\n",
    "\n",
    "Products file takes up about 1.2 MB of space, so the main focus of preprocessing won't be to save memory or disk space by normalizing. The main focus will be on cleaning the data. I won't go into very details of the decision making of the cleaning as it isn't the subject of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   itemid                 product_name sim_product_category  \\\n0   53048  Wildlachs in Spinatrahmsoße                 Fish   \n1   11274             Hummercremesuppe           Ready-made   \n2   30430              Lachscremesuppe           Ready-made   \n\n  sap_product_category                         declaration_of_ingredients  \\\n0                Fisch  Wildlachs 43,7%, Wasser, Vollmilch, Sahne 8,1%...   \n1                Fisch                                                NaN   \n2                Fisch  Wasser, Sahne 18%, Fischfond (Wasser, Fischext...   \n\n  nutriscore country_kitchen  \\\n0          B     Deutschland   \n1        NaN     Deutschland   \n2          C     Deutschland   \n\n                                         description main_category_1  \\\n0  Holen Sie sich ein echtes Gourmet-Menü nach Ha...           Fisch   \n1                                                NaN           Fisch   \n2                                                NaN           Fisch   \n\n  main_category_2  ...                                            who_sub  \\\n0             NaN  ...     {Frauentreff,\"Zeit zu Zweit\",Romantisch,weich}   \n1  Fertiggerichte  ...  {Frauentreff,Muttertag,\"Zeit zu Zweit\",Romanti...   \n2  Fertiggerichte  ...  {Schwanger,\"ohne Stücke\",Frauentreff,Romantisc...   \n\n                                when_main  \\\n0            {\"Mittagessen / Abendessen\"}   \n1  {\"Mittagessen / Abendessen\",Frühstück}   \n2  {\"Mittagessen / Abendessen\",Frühstück}   \n\n                                         when_sub           how_main  \\\n0                    {\"zum Wein\",\"großer Hunger\"}  {Alltag,Festlich}   \n1  {Brunch,\"zum Wein\",Sektfrühstück,Frühschoppen}   {Party,Festlich}   \n2      {Brunch,\"zum Wein\",\"kleiner Hunger\",Snack}   {Party,Festlich}   \n\n                                 how_sub           where_main where_sub  \\\n0       {Kochen,Hauptgang,Zwischengänge}  {\"Lunch aufwärmen\"}        {}   \n1                 {Vorspeise,Fingerfood}                   {}        {}   \n2  {Vorspeise,Zwischengänge,Mitternacht}                   {}        {}   \n\n                                           what_main  \\\n0            {Ostern,Karneval,Sommerlich,Winterlich}   \n1         {Ostern,\"Frühling \",Herbstlich,Winterlich}   \n2  {\"Frühling \",Ostern,Karneval,Sommerlich,Winter...   \n\n                                            what_sub  \\\n0  {Aschermittwoch,Osterfestessen,Advent,Karfreit...   \n1  {Erntezeit,Thanksgiving,Osterfestessen,Weihnac...   \n2  {Osterfestessen,Neujahrsfrühstück,Karfreitag,A...   \n\n                                      gluten_lactose  \n0  {\"Gluten \",keine_Laktose_Spuren,Laktose,keine_...  \n1                                                 {}  \n2                                                 {}  \n\n[3 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>itemid</th>\n      <th>product_name</th>\n      <th>sim_product_category</th>\n      <th>sap_product_category</th>\n      <th>declaration_of_ingredients</th>\n      <th>nutriscore</th>\n      <th>country_kitchen</th>\n      <th>description</th>\n      <th>main_category_1</th>\n      <th>main_category_2</th>\n      <th>...</th>\n      <th>who_sub</th>\n      <th>when_main</th>\n      <th>when_sub</th>\n      <th>how_main</th>\n      <th>how_sub</th>\n      <th>where_main</th>\n      <th>where_sub</th>\n      <th>what_main</th>\n      <th>what_sub</th>\n      <th>gluten_lactose</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>53048</td>\n      <td>Wildlachs in Spinatrahmsoße</td>\n      <td>Fish</td>\n      <td>Fisch</td>\n      <td>Wildlachs 43,7%, Wasser, Vollmilch, Sahne 8,1%...</td>\n      <td>B</td>\n      <td>Deutschland</td>\n      <td>Holen Sie sich ein echtes Gourmet-Menü nach Ha...</td>\n      <td>Fisch</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>{Frauentreff,\"Zeit zu Zweit\",Romantisch,weich}</td>\n      <td>{\"Mittagessen / Abendessen\"}</td>\n      <td>{\"zum Wein\",\"großer Hunger\"}</td>\n      <td>{Alltag,Festlich}</td>\n      <td>{Kochen,Hauptgang,Zwischengänge}</td>\n      <td>{\"Lunch aufwärmen\"}</td>\n      <td>{}</td>\n      <td>{Ostern,Karneval,Sommerlich,Winterlich}</td>\n      <td>{Aschermittwoch,Osterfestessen,Advent,Karfreit...</td>\n      <td>{\"Gluten \",keine_Laktose_Spuren,Laktose,keine_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11274</td>\n      <td>Hummercremesuppe</td>\n      <td>Ready-made</td>\n      <td>Fisch</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Deutschland</td>\n      <td>NaN</td>\n      <td>Fisch</td>\n      <td>Fertiggerichte</td>\n      <td>...</td>\n      <td>{Frauentreff,Muttertag,\"Zeit zu Zweit\",Romanti...</td>\n      <td>{\"Mittagessen / Abendessen\",Frühstück}</td>\n      <td>{Brunch,\"zum Wein\",Sektfrühstück,Frühschoppen}</td>\n      <td>{Party,Festlich}</td>\n      <td>{Vorspeise,Fingerfood}</td>\n      <td>{}</td>\n      <td>{}</td>\n      <td>{Ostern,\"Frühling \",Herbstlich,Winterlich}</td>\n      <td>{Erntezeit,Thanksgiving,Osterfestessen,Weihnac...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30430</td>\n      <td>Lachscremesuppe</td>\n      <td>Ready-made</td>\n      <td>Fisch</td>\n      <td>Wasser, Sahne 18%, Fischfond (Wasser, Fischext...</td>\n      <td>C</td>\n      <td>Deutschland</td>\n      <td>NaN</td>\n      <td>Fisch</td>\n      <td>Fertiggerichte</td>\n      <td>...</td>\n      <td>{Schwanger,\"ohne Stücke\",Frauentreff,Romantisc...</td>\n      <td>{\"Mittagessen / Abendessen\",Frühstück}</td>\n      <td>{Brunch,\"zum Wein\",\"kleiner Hunger\",Snack}</td>\n      <td>{Party,Festlich}</td>\n      <td>{Vorspeise,Zwischengänge,Mitternacht}</td>\n      <td>{}</td>\n      <td>{}</td>\n      <td>{\"Frühling \",Ostern,Karneval,Sommerlich,Winter...</td>\n      <td>{Osterfestessen,Neujahrsfrühstück,Karfreitag,A...</td>\n      <td>{}</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df = pd.read_csv('data/items.csv')\n",
    "products_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_data_columns(df, threshold = 0.5):\n",
    "    \"\"\" Drop column of data when more data than threshold percentage is missing.\"\"\"\n",
    "    return df.drop(columns=df.columns[(df.count() < threshold * len(df))].to_list())\n",
    "\n",
    "def drop_columns_matching_regex(df, rxp):\n",
    "    \"\"\" Drop columns of data where the name of the column matches with passed regular expression.\"\"\"\n",
    "    return df.drop(columns=df.columns[df.columns.str.contains(rxp, regex=True)].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "products = (\n",
    "    products_df.pipe(drop_missing_data_columns, threshold = 0.5)\n",
    "    .pipe(drop_columns_matching_regex, rxp='_main|_sub')\n",
    "    .drop(columns = ['declaration_of_ingredients', 'description', 'allergens', 'special_ingredients'])\n",
    "    .rename(columns={'itemid':'id'})\n",
    "    # Convert features with less than 60 unique values to categorical features\n",
    "    .astype(\n",
    "        {\n",
    "            **dict.fromkeys(df.columns[(df.nunique() < 60)].to_list(), 'category'),\n",
    "            **{'id':'uint16'}\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Uncomment next line to save products table\n",
    "# REQUIRES fastparquet package\n",
    "#products.to_parquet('data/products.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entity set\n",
    "\n",
    "After preprocessing and cleaning the data we can create an EntitySet and add dataframes and build relationships between them. (still working with smaller dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import featuretools.primitives as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed data or skip if in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to read data from saved files\n",
    "orders = pd.read_parquet('data/100k_orders.parquet')\n",
    "orders_products = pd.read_parquet('data/100k_orders_products.parquet')\n",
    "products = pd.read_parquet('data/products.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decided to sample the data to determine best features. Later when best features are established I will rerun the training on full data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Sample 5000 users\n",
    "orders_sample = orders[orders['user_id'].isin(orders['user_id'].drop_duplicates().sample(5000))]\n",
    "orders_products_sample = orders_products[orders_products['order_id'].isin(orders_sample['id'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instance of `EntitySet`. Using the `add_dataframe` method I added our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Entityset: sample_dataset\n  DataFrames:\n    orders [Rows: 85408, Columns: 3]\n    orders_products [Rows: 426108, Columns: 3]\n    products [Rows: 1426, Columns: 11]\n  Relationships:\n    No relationships"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(id='sample_dataset')\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='orders',\n",
    "    dataframe=orders_sample,\n",
    "    index='id',\n",
    "    time_index='date'\n",
    ")\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='orders_products',\n",
    "    dataframe=orders_products_sample,\n",
    "    index='id'\n",
    ")\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='products',\n",
    "    dataframe=products,\n",
    "    index='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are no relationships between the DataFrames. There is `add_relationship` method to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es.add_relationship('products', 'id', 'orders_products', 'product_id')\n",
    "es.add_relationship('orders', 'id', 'orders_products', 'order_id')\n",
    "\n",
    "# Create table of users out orders using normalization\n",
    "es.normalize_dataframe('orders','users','user_id')\n",
    "es.add_last_time_indexes(['users'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the structure of `EntitySet` I used `plot` method. It shows format of all the dataframes and relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 3.0.0 (20220226.1711)\n -->\n<!-- Title: sample_dataset Pages: 1 -->\n<svg width=\"486pt\" height=\"446pt\"\n viewBox=\"0.00 0.00 486.00 446.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 442)\">\n<title>sample_dataset</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-442 482,-442 482,4 -4,4\"/>\n<!-- orders -->\n<g id=\"node1\" class=\"node\">\n<title>orders</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-166 0,-257 243,-257 243,-166 0,-166\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-241.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">orders (85408 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-234 243,-234 \"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-218.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">date : Datetime; time_index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-203.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-188.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-173.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- users -->\n<g id=\"node4\" class=\"node\">\n<title>users</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-76.5 243,-76.5 243,-0.5 0,-0.5\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">users (5000 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-53.5 243,-53.5 \"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-38.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id : Integer; index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">first_orders_time : Datetime; time_index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- orders&#45;&gt;users -->\n<g id=\"edge3\" class=\"edge\">\n<title>orders&#45;&gt;users</title>\n<path fill=\"none\" stroke=\"black\" d=\"M121.5,-165.59C121.5,-165.59 121.5,-86.72 121.5,-86.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"125,-86.72 121.5,-76.72 118,-86.72 125,-86.72\"/>\n<text text-anchor=\"middle\" x=\"100.5\" y=\"-129.96\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id</text>\n</g>\n<!-- orders_products -->\n<g id=\"node2\" class=\"node\">\n<title>orders_products</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"124,-346.5 124,-437.5 367,-437.5 367,-346.5 124,-346.5\"/>\n<text text-anchor=\"middle\" x=\"245.5\" y=\"-422.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">orders_products (426108 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"124,-414.5 367,-414.5 \"/>\n<text text-anchor=\"start\" x=\"132\" y=\"-399.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">order_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-384.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">product_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-369.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-354.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- orders_products&#45;&gt;orders -->\n<g id=\"edge2\" class=\"edge\">\n<title>orders_products&#45;&gt;orders</title>\n<path fill=\"none\" stroke=\"black\" d=\"M183.5,-346.45C183.5,-346.45 183.5,-267.03 183.5,-267.03\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"187,-267.03 183.5,-257.03 180,-267.03 187,-267.03\"/>\n<text text-anchor=\"middle\" x=\"144\" y=\"-310.54\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id &#45;&gt; order_id</text>\n</g>\n<!-- products -->\n<g id=\"node3\" class=\"node\">\n<title>products</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"261,-113.5 261,-309.5 478,-309.5 478,-113.5 261,-113.5\"/>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-294.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">products (1426 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"261,-286.5 478,-286.5 \"/>\n<text text-anchor=\"start\" x=\"269\" y=\"-271.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-256.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">product_name : Unknown</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-241.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sim_product_category : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-226.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sap_product_category : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-211.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nutriscore : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-196.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">country_kitchen : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-181.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">main_category_1 : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-166.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sub_category_1 : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-151.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">vegetarian : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-136.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">type_of_meat : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-121.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gluten_lactose : Categorical</text>\n</g>\n<!-- orders_products&#45;&gt;products -->\n<g id=\"edge1\" class=\"edge\">\n<title>orders_products&#45;&gt;products</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314,-346.45C314,-346.45 314,-319.55 314,-319.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"317.5,-319.55 314,-309.55 310.5,-319.55 317.5,-319.55\"/>\n<text text-anchor=\"middle\" x=\"267\" y=\"-321.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id &#45;&gt; product_id</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x153a8e1aaf0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EntitySet` allows to set interesting values that will later be used in `where` clauses. I add interesting values using `add_interesting_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:06:17,777 featuretools.entityset - INFO    Column sim_product_category: Marking Ready-made as an interesting value\n",
      "2022-04-12 12:06:17,793 featuretools.entityset - INFO    Column sim_product_category: Marking Ice-cream as an interesting value\n",
      "2022-04-12 12:06:17,793 featuretools.entityset - INFO    Column sim_product_category: Marking Bakery as an interesting value\n",
      "2022-04-12 12:06:17,808 featuretools.entityset - INFO    Column sim_product_category: Marking Vegetables as an interesting value\n",
      "2022-04-12 12:06:17,808 featuretools.entityset - INFO    Column sim_product_category: Marking Fish as an interesting value\n",
      "2022-04-12 12:06:17,808 featuretools.entityset - INFO    Column sim_product_category: Marking Non-frozen food as an interesting value\n",
      "2022-04-12 12:06:17,808 featuretools.entityset - INFO    Column sim_product_category: Marking Other frozen food as an interesting value\n",
      "2022-04-12 12:06:17,808 featuretools.entityset - INFO    Column sim_product_category: Marking Meat as an interesting value\n",
      "2022-04-12 12:06:17,824 featuretools.entityset - INFO    Column sap_product_category: Marking Fertiggerichte as an interesting value\n",
      "2022-04-12 12:06:17,824 featuretools.entityset - INFO    Column sap_product_category: Marking Eis as an interesting value\n",
      "2022-04-12 12:06:17,824 featuretools.entityset - INFO    Column sap_product_category: Marking Gemüse as an interesting value\n",
      "2022-04-12 12:06:17,824 featuretools.entityset - INFO    Column sap_product_category: Marking Fisch as an interesting value\n",
      "2022-04-12 12:06:17,824 featuretools.entityset - INFO    Column sap_product_category: Marking nicht TK as an interesting value\n",
      "2022-04-12 12:06:17,824 featuretools.entityset - INFO    Column sap_product_category: Marking Backwaren as an interesting value\n",
      "2022-04-12 12:06:17,839 featuretools.entityset - INFO    Column sap_product_category: Marking non-food as an interesting value\n",
      "2022-04-12 12:06:17,839 featuretools.entityset - INFO    Column sap_product_category: Marking Fleisch as an interesting value\n",
      "2022-04-12 12:06:17,839 featuretools.entityset - INFO    Column nutriscore: Marking B as an interesting value\n",
      "2022-04-12 12:06:17,855 featuretools.entityset - INFO    Column nutriscore: Marking C as an interesting value\n",
      "2022-04-12 12:06:17,855 featuretools.entityset - INFO    Column nutriscore: Marking D as an interesting value\n",
      "2022-04-12 12:06:17,855 featuretools.entityset - INFO    Column nutriscore: Marking A as an interesting value\n",
      "2022-04-12 12:06:17,871 featuretools.entityset - INFO    Column nutriscore: Marking E as an interesting value\n",
      "2022-04-12 12:06:17,871 featuretools.entityset - INFO    Column nutriscore: Marking D|D|D as an interesting value\n",
      "2022-04-12 12:06:17,871 featuretools.entityset - INFO    Column nutriscore: Marking D|D as an interesting value\n",
      "2022-04-12 12:06:17,871 featuretools.entityset - INFO    Column nutriscore: Marking B|C as an interesting value\n",
      "2022-04-12 12:06:17,886 featuretools.entityset - INFO    Column country_kitchen: Marking Deutschland as an interesting value\n",
      "2022-04-12 12:06:17,902 featuretools.entityset - INFO    Column country_kitchen: Marking Italien as an interesting value\n",
      "2022-04-12 12:06:17,902 featuretools.entityset - INFO    Column main_category_1: Marking Fertiggerichte as an interesting value\n",
      "2022-04-12 12:06:17,902 featuretools.entityset - INFO    Column main_category_1: Marking Fleisch as an interesting value\n",
      "2022-04-12 12:06:17,918 featuretools.entityset - INFO    Column main_category_1: Marking Eis as an interesting value\n",
      "2022-04-12 12:06:17,918 featuretools.entityset - INFO    Column main_category_1: Marking Fisch as an interesting value\n",
      "2022-04-12 12:06:17,918 featuretools.entityset - INFO    Column main_category_1: Marking Gemüse as an interesting value\n",
      "2022-04-12 12:06:17,964 featuretools.entityset - INFO    Column main_category_1: Marking Torten_Kuchen as an interesting value\n",
      "2022-04-12 12:06:17,964 featuretools.entityset - INFO    Column main_category_1: Marking Pizza_Snacks as an interesting value\n",
      "2022-04-12 12:06:17,964 featuretools.entityset - INFO    Column main_category_1: Marking Wein & Spirituosen as an interesting value\n",
      "2022-04-12 12:06:17,964 featuretools.entityset - INFO    Column sub_category_1: Marking Pfannengerichte as an interesting value\n",
      "2022-04-12 12:06:18,011 featuretools.entityset - INFO    Column vegetarian: Marking f as an interesting value\n",
      "2022-04-12 12:06:18,011 featuretools.entityset - INFO    Column vegetarian: Marking t as an interesting value\n",
      "2022-04-12 12:06:18,011 featuretools.entityset - INFO    Column type_of_meat: Marking {} as an interesting value\n",
      "2022-04-12 12:06:18,011 featuretools.entityset - INFO    Column type_of_meat: Marking {Geflügel/-fett} as an interesting value\n",
      "2022-04-12 12:06:18,011 featuretools.entityset - INFO    Column type_of_meat: Marking {Fisch} as an interesting value\n",
      "2022-04-12 12:06:18,011 featuretools.entityset - INFO    Column type_of_meat: Marking {Schweinefleisch/-fett} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {\"Gluten \",keine_Laktose_Spuren,Laktose,keine_Weizen_Spuren,keine_Gluten_Spuren,Weizen} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {keine_Laktose,keine_Laktose_Spuren,kein_Weizen,keine_Weizen_Spuren,keine_Gluten_Spuren,\"kein_Gluten \"} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {keine_Laktose_Spuren,kein_Weizen,Laktose,keine_Weizen_Spuren,keine_Gluten_Spuren,\"kein_Gluten \"} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {\"Gluten \",keine_Laktose,keine_Laktose_Spuren,keine_Weizen_Spuren,keine_Gluten_Spuren,Weizen} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {keine_Laktose_Spuren,kein_Weizen,Laktose,\"kein_Gluten \",Weizen_Spuren,Gluten_Spuren} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {\"Gluten \",keine_Laktose_Spuren,kein_Weizen,Laktose,keine_Weizen_Spuren,keine_Gluten_Spuren} as an interesting value\n",
      "2022-04-12 12:06:18,058 featuretools.entityset - INFO    Column gluten_lactose: Marking {\"Gluten \",Laktose_Spuren,keine_Laktose,keine_Weizen_Spuren,keine_Gluten_Spuren,Weizen} as an interesting value\n"
     ]
    }
   ],
   "source": [
    "es.add_interesting_values(max_values=8,\n",
    "                          verbose=True,\n",
    "                          dataframe_name='products')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling time\n",
    "\n",
    "Since I want to calculate features for each user at certain point in time(before their order), cutoff time dataframe has to be created. Cutoff dataframe has to have targeted feature(i.e. `user_id`) and time feature(i.e. `date`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id       time\n16       4200354 2019-01-02\n23       1618203 2019-01-02\n29       7982007 2019-01-02\n35       1733805 2019-01-02\n45       7998635 2019-01-02\n...          ...        ...\n1711818  2404910 2020-12-30\n1711819  8133098 2020-12-30\n1711824  8088761 2020-12-30\n1711862  6659115 2020-12-30\n1711875  5086174 2020-12-30\n\n[85408 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16</th>\n      <td>4200354</td>\n      <td>2019-01-02</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1618203</td>\n      <td>2019-01-02</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7982007</td>\n      <td>2019-01-02</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1733805</td>\n      <td>2019-01-02</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>7998635</td>\n      <td>2019-01-02</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1711818</th>\n      <td>2404910</td>\n      <td>2020-12-30</td>\n    </tr>\n    <tr>\n      <th>1711819</th>\n      <td>8133098</td>\n      <td>2020-12-30</td>\n    </tr>\n    <tr>\n      <th>1711824</th>\n      <td>8088761</td>\n      <td>2020-12-30</td>\n    </tr>\n    <tr>\n      <th>1711862</th>\n      <td>6659115</td>\n      <td>2020-12-30</td>\n    </tr>\n    <tr>\n      <th>1711875</th>\n      <td>5086174</td>\n      <td>2020-12-30</td>\n    </tr>\n  </tbody>\n</table>\n<p>85408 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = orders_sample[['user_id','date']].rename(columns={'date':'time'})\n",
    "cutoff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating features and calculating the features matrix\n",
    "\n",
    "As mentioned feature engineering will be done using `featuretools` package and their `dfs` deep feature synthesis algorithm. Parameters of `dfs` can be found here: https://featuretools.alteryx.com/en/stable/generated/featuretools.dfs.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calculate_features(entityset, cutoff, training_window=None, features_only=False, n_jobs=1):\n",
    "    return ft.dfs(entityset=entityset,\n",
    "                    target_dataframe_name='users',\n",
    "                    agg_primitives=[\n",
    "                        'max', 'mean', 'count',\n",
    "                        'num_unique', 'mode', pr.AvgTimeBetween(unit='days'),\n",
    "                        'n_most_common', 'trend'\n",
    "                    ],\n",
    "                    trans_primitives=[\n",
    "                        'month','weekday', 'is_weekend',\n",
    "                        'time_since_previous'\n",
    "                    ],\n",
    "                    cutoff_time=cutoff,\n",
    "                    training_window=training_window,\n",
    "                    max_depth=3,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True,\n",
    "                    features_only=features_only,\n",
    "                    include_cutoff_time=False # Do not count on orders that came in same day\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = calculate_features(es, cutoff, features_only=True)\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling to larger dataset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac414b804d36e4c085e24a0481bd4396ee16fd0a7c5871f3d68efff1fa8b72ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}