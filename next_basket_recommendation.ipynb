{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next basket recommendation\n",
    "\n",
    "The task of next basket recommendation is to predict a content of customers basket at their future purchase.\n",
    "\n",
    "Task has been assigned as a competition among the students who took the Algorithms of data mining course at Faculty of Information Technology @ Czech Technical University in Prague.\n",
    "\n",
    "### Scoring function\n",
    "\n",
    "The competition has been divided into 2 rounds. The difference between rounds is the used scoring function. 1st round scoring function is Jaccard similarity coefficient\n",
    "\n",
    "$J(A,B) = \\frac{ |A \\cap B| }{ |A \\cup B| }$,\n",
    "\n",
    "where A is a real basket and B is a predicted one. Final score is a mean of $J(A,B)$ over all predictions. 2nd round scoring function is Generalized Jaccard similarity coefficient over multisets\n",
    "\n",
    "$J_g(A,B) = \\frac{ \\sum_i min(a_i, b_i) }{ \\sum_i max(a_i, b_i) }$,\n",
    "\n",
    "where $A$ is a real basket and $B$ is a predicted one. $a_i$ (resp. $b_i$) is number of occurences of $i$-th in $A$ (resp. $B$). Final score is a mean of $J_g(A,B)$ over all predictions. Generalized jaccard index takes into account cases where there is multiple occurrences of same products in one basket. Therefore the scoring function is stricter.\n",
    "\n",
    "Score in 1st rnd: TBA\n",
    "\n",
    "Position among others 1st rnd: TBA\n",
    "\n",
    "Score in 2nd rnd: TBA\n",
    "\n",
    "Position among others 2nd rnd: TBA\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Unfortunately I cannot provide the data for the problem as to avoid any legal issues. The data has been provided by a external company in collaboration with the university.\n",
    "\n",
    "\n",
    "\n",
    "Even though I cannot provide the dataset I will still try to capture my thought process and all the ideas and examples will be shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We have been provided with 2 types of csv files.\n",
    "\n",
    "First file contains 3 columns that together create a order history. The columns are `userid`, `date` and `itemids`. Columns are self-explanatory. `userid` is an ID number of an user, `date` is a date of a purchase and `itemids` is a space separated list of product IDs. Order history files come in 2 sizes. First, the smaller one, contains 1 700 000+ rows. Second file contains total of 8 800 000+ of data.\n",
    "\n",
    "| userid \t| date       \t| itemids           \t|\n",
    "|--------\t|------------\t|-------------------\t|\n",
    "| 12345  \t| 1995-30-7  \t| 11111 22222       \t|\n",
    "| 777777 \t| 2022-1-1   \t| 12314             \t|\n",
    "| 425645 \t| 2020-12-31 \t| 45646 46511 11111 \t|\n",
    "\n",
    "Second file contains information about products. Each row represents one product. There is 25+ features about each product. Total number of products is 1426. The thoughts about feature selection for products is described later.\n",
    "\n",
    "| productid \t| features.... \t|\n",
    "|-----------\t|--------------\t|\n",
    "| 11111     \t| features     \t|\n",
    "| 22222     \t| features     \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First, I work with smaller dataset. Scaling to full dataset will be done in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used to preprocess the data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this project I decided to use Featuretools package for automated feature engineering. Featuretools works with typical pandas dataframes, but it's recommended to use `EntitySet` class when working with tabular data with relationships between them. From the previous look at the data We know that there is many to many relationship between order history and products table. Many to many relationships are better decomposed (normalized) to 2 one to many relationships. Now let's separate the data into 3 tables. <b>Orders</b>, <b>Orders Products</b> and <b>Products</b> tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>date</th>\n",
       "      <th>itemids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7226385</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>42203 41183 15823 39620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7226385</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>54231 14939 39462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7226385</td>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>15823 21028 39620 52846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userid        date                  itemids\n",
       "0  7226385  2019-01-22  42203 41183 15823 39620\n",
       "1  7226385  2019-02-12        54231 14939 39462\n",
       "2  7226385  2019-03-11  15823 21028 39620 52846"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train100k.csv') # Read smaller dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale later to bigger dataset I will create function to normalize the train file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_file(df, return_labels = True):\n",
    "    \"\"\" Normalize train file into Orders and Orders_Products table. \"\"\"\n",
    "    orders = ( \n",
    "        df.assign(                 \n",
    "                date=lambda x: pd.to_datetime(x['date'], infer_datetime_format=True),\n",
    "        )                \n",
    "        .rename(columns={'userid':'user_id'})\n",
    "        # Sort orders by date and create unique id for every order\n",
    "        .set_index(['date'])\n",
    "        .sort_index()\n",
    "        .assign(\n",
    "            id=range(0,len(df))\n",
    "        )\n",
    "        # Change types to save memory\n",
    "        .astype({'user_id':'uint32','id':'uint32'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    # We keep itemids column in orders dataframe to keep the relationship of order id with products\n",
    "\n",
    "    # labels will be later used to train multilabel classifiers - do not mind them for now\n",
    "    labels = orders[['id', 'itemids']]\n",
    "\n",
    "    orders_products = ( \n",
    "        orders.drop(columns=['user_id','date'])\n",
    "        # Split and explode itemids list so that each product will be on individual row, but keep order id\n",
    "        .assign(product_id=lambda x: x['itemids'].str.split())\n",
    "        .rename(columns={'id':'order_id'})\n",
    "        .drop(columns='itemids')\n",
    "        .explode('product_id')\n",
    "        .astype({'product_id':'uint16'})        \n",
    "        .assign(\n",
    "            id=lambda x: range(0,len(x)) # Creates unique id for every order product pair\n",
    "        )\n",
    "        .astype({'id':'uint32'})\n",
    "    )\n",
    "    orders_products.index = pd.RangeIndex(len(orders_products.index))\n",
    "    \n",
    "    # Lastly drop itemids from orders\n",
    "    orders = orders.drop(columns=['itemids'])\n",
    "\n",
    "    if return_labels:\n",
    "        return orders, orders_products, labels\n",
    "    return orders, orders_products\n",
    "\n",
    "\n",
    "orders, orders_products, labels = normalize_train_file(df, return_labels = True)\n",
    "\n",
    "# Uncomment to save normalized tables for later use\n",
    "# REQUIRES fastparquet package\n",
    "#orders.to_parquet(f'data/100k_orders.parquet',engine='fastparquet')\n",
    "#orders_products.to_parquet(f'data/100k_orders_products.parquet',engine='fastparquet')\n",
    "#labels.to_parquet(f'data/100k_labels.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of products file\n",
    "\n",
    "Products file takes up about 1.2 MB of space, so the main focus of preprocessing won't be to save memory or disk space by normalizing. The main focus will be on cleaning the data. I won't go into very details of the decision making of the cleaning as it isn't the subject of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>product_name</th>\n",
       "      <th>sim_product_category</th>\n",
       "      <th>sap_product_category</th>\n",
       "      <th>declaration_of_ingredients</th>\n",
       "      <th>nutriscore</th>\n",
       "      <th>country_kitchen</th>\n",
       "      <th>description</th>\n",
       "      <th>main_category_1</th>\n",
       "      <th>main_category_2</th>\n",
       "      <th>...</th>\n",
       "      <th>who_sub</th>\n",
       "      <th>when_main</th>\n",
       "      <th>when_sub</th>\n",
       "      <th>how_main</th>\n",
       "      <th>how_sub</th>\n",
       "      <th>where_main</th>\n",
       "      <th>where_sub</th>\n",
       "      <th>what_main</th>\n",
       "      <th>what_sub</th>\n",
       "      <th>gluten_lactose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53048</td>\n",
       "      <td>Wildlachs in Spinatrahmsoße</td>\n",
       "      <td>Fish</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Wildlachs 43,7%, Wasser, Vollmilch, Sahne 8,1%...</td>\n",
       "      <td>B</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>Holen Sie sich ein echtes Gourmet-Menü nach Ha...</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>{Frauentreff,\"Zeit zu Zweit\",Romantisch,weich}</td>\n",
       "      <td>{\"Mittagessen / Abendessen\"}</td>\n",
       "      <td>{\"zum Wein\",\"großer Hunger\"}</td>\n",
       "      <td>{Alltag,Festlich}</td>\n",
       "      <td>{Kochen,Hauptgang,Zwischengänge}</td>\n",
       "      <td>{\"Lunch aufwärmen\"}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Ostern,Karneval,Sommerlich,Winterlich}</td>\n",
       "      <td>{Aschermittwoch,Osterfestessen,Advent,Karfreit...</td>\n",
       "      <td>{\"Gluten \",keine_Laktose_Spuren,Laktose,keine_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11274</td>\n",
       "      <td>Hummercremesuppe</td>\n",
       "      <td>Ready-made</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Fertiggerichte</td>\n",
       "      <td>...</td>\n",
       "      <td>{Frauentreff,Muttertag,\"Zeit zu Zweit\",Romanti...</td>\n",
       "      <td>{\"Mittagessen / Abendessen\",Frühstück}</td>\n",
       "      <td>{Brunch,\"zum Wein\",Sektfrühstück,Frühschoppen}</td>\n",
       "      <td>{Party,Festlich}</td>\n",
       "      <td>{Vorspeise,Fingerfood}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Ostern,\"Frühling \",Herbstlich,Winterlich}</td>\n",
       "      <td>{Erntezeit,Thanksgiving,Osterfestessen,Weihnac...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30430</td>\n",
       "      <td>Lachscremesuppe</td>\n",
       "      <td>Ready-made</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Wasser, Sahne 18%, Fischfond (Wasser, Fischext...</td>\n",
       "      <td>C</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fisch</td>\n",
       "      <td>Fertiggerichte</td>\n",
       "      <td>...</td>\n",
       "      <td>{Schwanger,\"ohne Stücke\",Frauentreff,Romantisc...</td>\n",
       "      <td>{\"Mittagessen / Abendessen\",Frühstück}</td>\n",
       "      <td>{Brunch,\"zum Wein\",\"kleiner Hunger\",Snack}</td>\n",
       "      <td>{Party,Festlich}</td>\n",
       "      <td>{Vorspeise,Zwischengänge,Mitternacht}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"Frühling \",Ostern,Karneval,Sommerlich,Winter...</td>\n",
       "      <td>{Osterfestessen,Neujahrsfrühstück,Karfreitag,A...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemid                 product_name sim_product_category  \\\n",
       "0   53048  Wildlachs in Spinatrahmsoße                 Fish   \n",
       "1   11274             Hummercremesuppe           Ready-made   \n",
       "2   30430              Lachscremesuppe           Ready-made   \n",
       "\n",
       "  sap_product_category                         declaration_of_ingredients  \\\n",
       "0                Fisch  Wildlachs 43,7%, Wasser, Vollmilch, Sahne 8,1%...   \n",
       "1                Fisch                                                NaN   \n",
       "2                Fisch  Wasser, Sahne 18%, Fischfond (Wasser, Fischext...   \n",
       "\n",
       "  nutriscore country_kitchen  \\\n",
       "0          B     Deutschland   \n",
       "1        NaN     Deutschland   \n",
       "2          C     Deutschland   \n",
       "\n",
       "                                         description main_category_1  \\\n",
       "0  Holen Sie sich ein echtes Gourmet-Menü nach Ha...           Fisch   \n",
       "1                                                NaN           Fisch   \n",
       "2                                                NaN           Fisch   \n",
       "\n",
       "  main_category_2  ...                                            who_sub  \\\n",
       "0             NaN  ...     {Frauentreff,\"Zeit zu Zweit\",Romantisch,weich}   \n",
       "1  Fertiggerichte  ...  {Frauentreff,Muttertag,\"Zeit zu Zweit\",Romanti...   \n",
       "2  Fertiggerichte  ...  {Schwanger,\"ohne Stücke\",Frauentreff,Romantisc...   \n",
       "\n",
       "                                when_main  \\\n",
       "0            {\"Mittagessen / Abendessen\"}   \n",
       "1  {\"Mittagessen / Abendessen\",Frühstück}   \n",
       "2  {\"Mittagessen / Abendessen\",Frühstück}   \n",
       "\n",
       "                                         when_sub           how_main  \\\n",
       "0                    {\"zum Wein\",\"großer Hunger\"}  {Alltag,Festlich}   \n",
       "1  {Brunch,\"zum Wein\",Sektfrühstück,Frühschoppen}   {Party,Festlich}   \n",
       "2      {Brunch,\"zum Wein\",\"kleiner Hunger\",Snack}   {Party,Festlich}   \n",
       "\n",
       "                                 how_sub           where_main where_sub  \\\n",
       "0       {Kochen,Hauptgang,Zwischengänge}  {\"Lunch aufwärmen\"}        {}   \n",
       "1                 {Vorspeise,Fingerfood}                   {}        {}   \n",
       "2  {Vorspeise,Zwischengänge,Mitternacht}                   {}        {}   \n",
       "\n",
       "                                           what_main  \\\n",
       "0            {Ostern,Karneval,Sommerlich,Winterlich}   \n",
       "1         {Ostern,\"Frühling \",Herbstlich,Winterlich}   \n",
       "2  {\"Frühling \",Ostern,Karneval,Sommerlich,Winter...   \n",
       "\n",
       "                                            what_sub  \\\n",
       "0  {Aschermittwoch,Osterfestessen,Advent,Karfreit...   \n",
       "1  {Erntezeit,Thanksgiving,Osterfestessen,Weihnac...   \n",
       "2  {Osterfestessen,Neujahrsfrühstück,Karfreitag,A...   \n",
       "\n",
       "                                      gluten_lactose  \n",
       "0  {\"Gluten \",keine_Laktose_Spuren,Laktose,keine_...  \n",
       "1                                                 {}  \n",
       "2                                                 {}  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df = pd.read_csv('data/items.csv')\n",
    "products_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_data_columns(df, threshold = 0.5):\n",
    "    \"\"\" Drop column of data when more data than threshold percentage is missing.\"\"\"\n",
    "    return df.drop(columns=df.columns[(df.count() < threshold * len(df))].to_list())\n",
    "\n",
    "def drop_columns_matching_regex(df, rxp):\n",
    "    \"\"\" Drop columns of data where the name of the column matches with passed regular expression.\"\"\"\n",
    "    return df.drop(columns=df.columns[df.columns.str.contains(rxp, regex=True)].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "products = (\n",
    "    products_df.pipe(drop_missing_data_columns, threshold = 0.5)\n",
    "    .pipe(drop_columns_matching_regex, rxp='_main|_sub')\n",
    "    .drop(columns = ['declaration_of_ingredients', 'description', 'allergens', 'special_ingredients'])\n",
    "    .rename(columns={'itemid':'id'})\n",
    "    # Convert features with less than 60 unique values to categorical features\n",
    "    .astype(\n",
    "        {\n",
    "            **dict.fromkeys(df.columns[(df.nunique() < 60)].to_list(), 'category'),\n",
    "            **{'id':'uint16'}\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Uncomment next line to save products table\n",
    "# REQUIRES fastparquet package\n",
    "#products.to_parquet('data/products.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entity set\n",
    "\n",
    "After preprocessing and cleaning the data we can create an EntitySet and add dataframes and build relationships between them. (still working with smaller dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import featuretools.primitives as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed data or skip if in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to read data from saved files\n",
    "orders = pd.read_parquet('data/100k_orders.parquet')\n",
    "orders_products = pd.read_parquet('data/100k_orders_products.parquet')\n",
    "products = pd.read_parquet('data/products.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instance of `EntitySet`. Using the `add_dataframe` method I added our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: smaller_dataset\n",
       "  DataFrames:\n",
       "    orders [Rows: 1711877, Columns: 3]\n",
       "    orders_products [Rows: 8454526, Columns: 3]\n",
       "    products [Rows: 1426, Columns: 11]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(id='smaller_dataset')\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='orders',\n",
    "    dataframe=orders,\n",
    "    index='id',\n",
    "    time_index='date'\n",
    ")\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='orders_products',\n",
    "    dataframe=orders_products,\n",
    "    index='id'\n",
    ")\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe_name='products',\n",
    "    dataframe=products,\n",
    "    index='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are no relationships between the DataFrames. There is `add_relationship` method to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es.add_relationship('products', 'id', 'orders_products', 'product_id')\n",
    "es.add_relationship('orders', 'id', 'orders_products', 'order_id')\n",
    "\n",
    "# Create table of users out orders using normalization\n",
    "es.normalize_dataframe('orders','users','user_id')\n",
    "es.add_last_time_indexes(['users'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the structure of `EntitySet` I used `plot` method. It shows format of all the dataframes and relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n -->\n<!-- Title: smaller_dataset Pages: 1 -->\n<svg width=\"486pt\" height=\"446pt\"\n viewBox=\"0.00 0.00 486.00 446.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 442)\">\n<title>smaller_dataset</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-442 482,-442 482,4 -4,4\"/>\n<!-- orders -->\n<g id=\"node1\" class=\"node\">\n<title>orders</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-166 0,-257 243,-257 243,-166 0,-166\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-241.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">orders (1711877 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-234 243,-234 \"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-218.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">date : Datetime; time_index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-203.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-188.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-173.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- users -->\n<g id=\"node4\" class=\"node\">\n<title>users</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-76.5 243,-76.5 243,-0.5 0,-0.5\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">users (100000 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-53.5 243,-53.5 \"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-38.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id : Integer; index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">first_orders_time : Datetime; time_index</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- orders&#45;&gt;users -->\n<g id=\"edge3\" class=\"edge\">\n<title>orders&#45;&gt;users</title>\n<path fill=\"none\" stroke=\"black\" d=\"M121.5,-165.59C121.5,-165.59 121.5,-86.72 121.5,-86.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"125,-86.72 121.5,-76.72 118,-86.72 125,-86.72\"/>\n<text text-anchor=\"middle\" x=\"100.5\" y=\"-129.96\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">user_id</text>\n</g>\n<!-- orders_products -->\n<g id=\"node2\" class=\"node\">\n<title>orders_products</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"124,-346.5 124,-437.5 367,-437.5 367,-346.5 124,-346.5\"/>\n<text text-anchor=\"middle\" x=\"245.5\" y=\"-422.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">orders_products (8454526 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"124,-414.5 367,-414.5 \"/>\n<text text-anchor=\"start\" x=\"132\" y=\"-399.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">order_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-384.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">product_id : Integer; foreign_key</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-369.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-354.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_ft_last_time : Datetime; last_time_index</text>\n</g>\n<!-- orders_products&#45;&gt;orders -->\n<g id=\"edge2\" class=\"edge\">\n<title>orders_products&#45;&gt;orders</title>\n<path fill=\"none\" stroke=\"black\" d=\"M183.5,-346.45C183.5,-346.45 183.5,-267.03 183.5,-267.03\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"187,-267.03 183.5,-257.03 180,-267.03 187,-267.03\"/>\n<text text-anchor=\"middle\" x=\"144\" y=\"-310.54\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id &#45;&gt; order_id</text>\n</g>\n<!-- products -->\n<g id=\"node3\" class=\"node\">\n<title>products</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"261,-113.5 261,-309.5 478,-309.5 478,-113.5 261,-113.5\"/>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-294.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">products (1426 rows)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"261,-286.5 478,-286.5 \"/>\n<text text-anchor=\"start\" x=\"269\" y=\"-271.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id : Integer; index</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-256.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">product_name : Unknown</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-241.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sim_product_category : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-226.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sap_product_category : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-211.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nutriscore : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-196.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">country_kitchen : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-181.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">main_category_1 : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-166.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sub_category_1 : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-151.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">vegetarian : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-136.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">type_of_meat : Categorical</text>\n<text text-anchor=\"start\" x=\"269\" y=\"-121.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gluten_lactose : Categorical</text>\n</g>\n<!-- orders_products&#45;&gt;products -->\n<g id=\"edge1\" class=\"edge\">\n<title>orders_products&#45;&gt;products</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314,-346.45C314,-346.45 314,-319.55 314,-319.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"317.5,-319.55 314,-309.55 310.5,-319.55 317.5,-319.55\"/>\n<text text-anchor=\"middle\" x=\"267\" y=\"-321.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id &#45;&gt; product_id</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1bcc2746490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EntitySet` allows to set interesting values that will later be used in `where` clauses. I add interesting values using `add_interesting_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es.add_interesting_values(dataframe_name=\"products\",\n",
    "                          values={\n",
    "                              'vegetarian': ['t', 'f'],\n",
    "                              'sim_product_category': ['Fish','Ready-made','Snacks', 'Non-food','Vegetables','Potato products','Poultry','Fruit','Other frozen food','Meat','Non-frozen food','Bakery','Ice-cream','Milk products','Desserts']\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling time\n",
    "\n",
    "Since I want to calculate features for each user at certain point in time, I have to create cutoff times. Calculating features for each user at every order is very expensive even with the size of the smaller dataset. There are 3 ways I will use to reduce the number of cutoff times. The method with the best result will be later used for large dataset. \n",
    "1. Take only last `n` order times of each customer.\n",
    "2. Take `n` random order times of each customer.\n",
    "3. Use only certain amount of data before each cutoff time. Defined by Timedelta passed as `training_window` argument to `featuretools.dfs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_cutoff(orders, method='all', n=None):\n",
    "    methods = ['all', 'last', 'sample']\n",
    "    if method not in methods:\n",
    "        raise Exception(f'Wrong method. Available methods: {methods}, passed method: {method}.')\n",
    "\n",
    "    cutoff = orders[['user_id','date']].rename(columns={'date':'time'})\n",
    "    if method == 'all':\n",
    "        return cutoff\n",
    "\n",
    "\n",
    "    if not n:\n",
    "        raise Exception(f'Passed method \"{method}\" requires parameter \"n\" to be set.')\n",
    "\n",
    "    cutoff = cutoff.groupby('user_id')\n",
    "    if method == 'last':\n",
    "        return cutoff.tail(n)\n",
    "    elif method == 'sample':\n",
    "        return cutoff.sample(n=n, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example below shows ~1.7 mil. cutoff times reduced to 300k. Although it is expected that prediction ability should be lower with less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59388</th>\n",
       "      <td>5527580</td>\n",
       "      <td>2019-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99593</th>\n",
       "      <td>6418931</td>\n",
       "      <td>2019-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108954</th>\n",
       "      <td>5527580</td>\n",
       "      <td>2019-02-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121106</th>\n",
       "      <td>5070067</td>\n",
       "      <td>2019-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121413</th>\n",
       "      <td>4885966</td>\n",
       "      <td>2019-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711872</th>\n",
       "      <td>4631009</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711873</th>\n",
       "      <td>8480443</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711874</th>\n",
       "      <td>8381298</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711875</th>\n",
       "      <td>5086174</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711876</th>\n",
       "      <td>6802440</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id       time\n",
       "59388    5527580 2019-01-25\n",
       "99593    6418931 2019-02-11\n",
       "108954   5527580 2019-02-14\n",
       "121106   5070067 2019-02-19\n",
       "121413   4885966 2019-02-19\n",
       "...          ...        ...\n",
       "1711872  4631009 2020-12-30\n",
       "1711873  8480443 2020-12-30\n",
       "1711874  8381298 2020-12-30\n",
       "1711875  5086174 2020-12-30\n",
       "1711876  6802440 2020-12-30\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = create_cutoff(orders, method='last', n=3)\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features and calculating the features matrix\n",
    "\n",
    "As mentioned feature engineering will be done using `featuretools` package and their `dfs` deep feature synthesis algorithm. Parameters of `dfs` can be found here: https://featuretools.alteryx.com/en/stable/generated/featuretools.dfs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_features(entityset, cutoff, training_window=None, features_only=False, n_jobs=1):\n",
    "    return ft.dfs(entityset=entityset,\n",
    "                    target_dataframe_name='users',\n",
    "                    agg_primitives=[\n",
    "                        'sum', 'max', 'mean', 'count',\n",
    "                        'num_unique', 'mode', pr.AvgTimeBetween(unit='days'),\n",
    "                    ],\n",
    "                    trans_primitives=[\n",
    "                        'month','weekday'\n",
    "                    ],\n",
    "                    cutoff_time=cutoff,\n",
    "                    training_window=training_window,\n",
    "                    approximate=ft.Timedelta(1, unit='d'),\n",
    "                    max_depth=4,\n",
    "                    n_jobs=n_jobs,\n",
    "                    ignore_columns={'users':['first_orders_time']},\n",
    "                    verbose=True,\n",
    "                    features_only=features_only,\n",
    "                    include_cutoff_time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = create_cutoff(orders, method='last', n=1)\n",
    "features = calculate_features(es, cutoff, features_only=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling to larger dataset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac414b804d36e4c085e24a0481bd4396ee16fd0a7c5871f3d68efff1fa8b72ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
